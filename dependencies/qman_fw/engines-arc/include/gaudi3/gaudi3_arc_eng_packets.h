/* SPDX-License-Identifier: MIT
 *
 * Copyright (C) 2020 HabanaLabs Ltd.
 * All Rights Reserved.
 */

#ifndef __GAUDI3_ARC_ENG_PACKETS_H__
#define __GAUDI3_ARC_ENG_PACKETS_H__

#include <stdint.h>
#include "gaudi3_arc_common_packets.h"

/**
 * \enum    eng_job_id_t
 * \brief   Ids of commonly used kernels.
 * \details  Ids of commonly used kernels.
 */
enum eng_job_id_t {
	/**<
	 * This is the id of a barrier job.
	 */
	ENG_BARRIER_JOB_ID = 0xE000,

	/**<
	 * This is the id of a NULL job.
	 * This is submitted when a specific engine does not
	 * have a valid work to do as part of Work distribution
	 * (or) SFG (or) MCID Rollover
	 */
	ENG_NULL_JOB_ID = 0xFFFF
};

/**
 * \enum    eng_arc_cmd_t
 * \brief   Various engine commands
 * \details Command type enum specifies the commands that are processed by
 *	    compute engines ARCs. These are part of ECB list.
 */
enum eng_arc_cmd_t {
	ECB_CMD_LIST_SIZE = 0,
	ECB_CMD_NOP = 1,
	ECB_CMD_WD_FENCE_AND_EXE = 2,
	ECB_CMD_SCHED_DMA = 3,
	ECB_CMD_STATIC_DESC_V2 = 4,
	ECB_CMD_SFG = 5,
	ECB_CMD_RESET_SOSET = 6,
	ECB_CMD_MCID_ROLLOVER = 7,
	ECB_CMD_COUNT = 8
};

/**
 * \enum    eng_compute_arc_cmd_t
 * \brief   Various opcodes for all compute engine arcs
 * \details Command IDs for commands sent to Compute Engine ARC from Scheduler ARC
 *	    These opcodes are internally generated by scheduler. Used by profiler
 *	    to display traces.
 *	    These enums are for internal usage of scheduler firmware and
 *	    kept here to prepare profiler event table. This should not be
 *	    used by any other components other than ARC firmware.
 */
enum eng_compute_arc_cmd_t {
	ENG_COMPUTE_ARC_CMD_BARRIER = 0x0,
	ENG_COMPUTE_ARC_CMD_ALLOC_BARRIER = 0x1,
	ENG_COMPUTE_ARC_CMD_DISPATCH_COMPUTE_ECB_LIST_V3 = 0x2,
	ENG_COMPUTE_ARC_CMD_UPDATE_RECIPE_BASE_V2 = 0x3,
	ENG_COMPUTE_ARC_CMD_COUNT = 0x4,
	ENG_COMPUTE_ARC_CMD_SIZE = 0x1F
};

/**
 * \enum    eng_pdma_arc_cmd_t
 * \brief   Various opcodes for all PDMA RX engine arc
 * \details Command IDs for commands sent to PDMA Engine ARC from Scheduler ARC
 *	    These opcodes are internally generated by scheduler. Used by profiler
 *	    to display traces.
 *	    These enums are for internal usage of scheduler firmware and
 *	    kept here to prepare profiler event table. This should not be
 *	    used by any other components other than ARC firmware.
 */
enum eng_pdma_arc_cmd_t {
	ENG_PDMA_ARC_CMD_BATCH_TRANSFER = 0x0,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode.
	 * Firmware does not split them into smaller chunk and submits that to PDMA engine
	 * as single transfer. Once submitted this transfer will run to completion
	 */
	ENG_PDMA_ARC_CMD_BATCH_CASTUP = 0x1,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode. This command
	 * supports cast up functionality along with optional reduction support.
	 * Firmware does not split them into smaller chunks and submits single transfer request
	 * to PDMA engine.
	 */
	ENG_PDMA_ARC_CMD_BATCH_USER_DATA = ENG_PDMA_ARC_CMD_BATCH_TRANSFER,
	/**<
	 */
	ENG_PDMA_ARC_CMD_BATCH_COMMANDS = ENG_PDMA_ARC_CMD_BATCH_TRANSFER,
	/**<
	 */
	ENG_PDMA_ARC_CMD_RX_USER_DATA = ENG_PDMA_ARC_CMD_BATCH_TRANSFER,
	/**<
	 */
	ENG_PDMA_ARC_CMD_TX_NETWORK_MEMOPS = ENG_PDMA_ARC_CMD_BATCH_TRANSFER,
	/**<
	 * Network applications should use this command to perform Memset/Memcopy
	 * operations with optional reduction support for Host to Device transfers
	 */
	ENG_PDMA_ARC_CMD_TX_NETWORK_CASTUP = ENG_PDMA_ARC_CMD_BATCH_CASTUP,
	/**<
	 * Network applications should use this command to perform Cast Up
	 * operations with Host to Device transfers. Reduction parameters must be
	 * set while doing cast up
	 */
	ENG_PDMA_ARC_CMD_RX_NETWORK_MEMOPS = ENG_PDMA_ARC_CMD_BATCH_TRANSFER,
	/**<
	 * Network applications should use this command to perform Memset/Memcopy
	 * operations for Device to Host and Device to Device transfers.
	 * Reduction wont work for Rx transfers as the Host memory controller
	 * does not support reduction
	 */
	ENG_PDMA_ARC_CMD_COUNT = 0x2,
	/**<
	 * Total number of PDMA commands
	 */
	ENG_PDMA_ARC_CMD_SIZE = 0x1F
};

/**
 * \enum    cme_ecb_list_cmd_t
 * \brief   Opcodes for commands contained in the ECB list
 * \details Opcodes for commands contained in the ECB list
 */
enum cme_ecb_list_cmd_t {
	CME_ECBL_CMD_DISCARD_CLS = 0x0,
	/**<
	 * Discard Cache Lines
	 */
	CME_ECBL_CMD_DEGRADE_CLS = 0x1,
	/**<
	 * Degrade Cache lines Class
	 */
	CME_ECBL_CMD_RESET_SOSET = 0x2,
	/**<
	 * Reset SO Set
	 */
	CME_ECBL_CMD_MCID_ROLLOVER = 0x3,
	/**<
	 * MCID Rollover of Discard MCID Range
	 */
	CME_ECBL_CMD_NOP = 0x4,
	/**<
	 * NOP command for padding
	 */
	CME_ECBL_CMD_COUNT = 0x5,
	CME_ECBL_CMD_SIZE = 0xF
};

/**
 * \enum    nic_eng_arc_cmd_t
 * \brief   Various NIC commands
 * \details Command type enum specifies the commands that are processed by
 *	    NIC engines ARCs.
 */
enum nic_eng_arc_cmd_t {
	NIC_CMD_UPDATE_GLBL_CTXT = 0,
	NIC_CMD_UPDATE_COLL_CTXT = 1,
	NIC_CMD_SEND_RECV_NOP = 2,
	NIC_CMD_COLL_OPS_LONG = 3,
	NIC_CMD_SEND_RECV = 4, /* NIC SEND/RECV command for user */
	NIC_CMD_SEND_RECV1 = 5,
	NIC_CMD_SEND_RECV2 = 6,
	NIC_CMD_SEND_RECV3 = 7,
	NIC_CMD_COLL_OPS = 8,
	NIC_CMD_COLL_OPS1 = 9,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS2 = 10, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS3 = 11, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS4 = 12, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS5 = 13, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS6 = 14, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS7 = 15,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_ENG_ARC_CMD_COUNT = 16,
	NIC_ENG_ARC_CMD_SIZE = 0xF
};

/**
 * \enum    nic_scaleout_eng_arc_cmd_t
 * \brief   Various NIC scaleout commands
 * \details Command type enum specifies the commands that are processed by
 *	    NIC engines ARCs.
 */
enum nic_scaleout_eng_arc_cmd_t {
	NIC_SCALEOUT_CMD_COLL_OPS = 0,
	NIC_SCALEOUT_ENG_ARC_CMD_COUNT = 1,
	NIC_SCALEOUT_ENG_ARC_CMD_SIZE = 0xF
};

/**
 * Total count of Work distribution context supported
 */
#define WD_CTXT_COUNT	8

#define MAX_DIMENSIONS	5

#define TENSOR_DIM0	0
#define TENSOR_DIM1	1
#define TENSOR_DIM2	2
#define TENSOR_DIM3	3
#define TENSOR_DIM4	4

/**
 * DCCM buffer size for doing the DMA transfers
 * All the ECB commands should be within these boundaries
 */

enum {
	STATIC_COMPUTE_ECB_LIST_BUFF_SIZE = 256,
	DYNAMIC_COMPUTE_ECB_LIST_BUFF_SIZE = 256,
};
/**
 * \struct  tpc_virt_sob_id_t
 * \brief   virtual sob identifier for TPC
 * \details Virtual SOB ID for TPC that is shared between GC and Firmware.
 * 	    Each SO Set of the TPC is divided into two sub so sets.
 * 	    Threshold value to arm the monitor is always in the multiple of 32
 * 	    After completing work, each TPC increments the same SOB. Since
 * 	    there are large number of TPC, we can not use different SOBs for
 *	    each TPCs.
 */
struct tpc_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold_v2:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/**<
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  mme_virt_sob_id_t
 * \brief   virtual sob identifier for MMEs
 * \details Virtual SOB ID for MMEs that is shared between GC and Firmware.
 * 	    Each SO Set of the MMEs is divided into two sub so sets.
 * 	    After completing work, each MME increments the its own SOB. Since
 * 	    there are only two master MMEs, we need two SOBs, there for each
 * 	    sub so set is grouped in pairs of two SOBs.
 * 	    SUB SO SET0 : [Pair0(SOB0, SOB1), Pair1(SOB2, SOB3),
 * 	                   Pair2(SOB4, SOB5), Pair3(SOB6, SOB7)],
 * 	    SUB SO SET1 : [Pair0(SOB8, SOB9), Pair1(SOB10, SOB11),
 * 		           Pair2(SOB12, SOB13) Pair3(SOB14, SOB15)]
 * 	    Due to this MME0 udates only SOBs with even offset and MME2 updates
 * 	    only odd offset SOBs
 * 	    one monitor monitors both the SOBs within a pair.
 */
struct mme_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:13;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t sob_pair_id:2;
			/**<
			 * SOBs within the sub so set is grouped into pair of
			 * two, as there are two MMEs. Each MME updates its own
			 * SOBs.
			 */
			uint16_t sub_soset_id:1;
			/**<
			 * one SO set of the MME is divided into two, known
			 * as sub so sets. This field specifies which sub so
			 * set to be used
			 */
		};
		struct {
			uint16_t threshold_v2:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/**<
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  mme_xpose_virt_sob_id_t
 * \brief   virtual sob identifier for MME Transpose Engines
 * \details Virtual SOB ID for MME Transpose that is shared between GC and Firmware.
 */
struct mme_xpose_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold_v2:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/**<
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  rot_virt_sob_id_t
 * \brief   virtual sob identifier for Rotators
 * \details Virtual SOB ID for Rotators that is shared between GC and
 *	    Firmware. Each SO set of the ROT is divided into eight sub so sets.
 *		After completing work, each ROT increments its own SOB.
 *		SUB SO SET0: SOB0(ROT0), SOB1(ROT1)
 */
struct rot_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:13;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t sub_soset_id:3;
			/**<
			 * one SO set of the ROT is divided into eight, known
			 * as sub so sets. This field specifies which sub so
			 * set to be used
			 */
		};
		struct {
			uint16_t threshold_v2:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/**<
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  dbg_virt_sob_id_t
 * \brief   virtual sob identifier for debug purpose
 * \details Virtual SOB ID for debug purpose that is shared between GC and
 *          Firmware.
 */
struct dbg_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/*
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  dbg_virt_long_sob_id_t
 * \brief   long virtual sob identifier for debug purpose
 * \details Long Virtual SOB ID for debug purpose that is shared between GC and
 *          Firmware.
 */
struct dbg_virt_long_sob_id_t {
	union {
		struct {
			uint32_t threshold:30;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint32_t reserved:2;
			/*
			 * reserved
			 */
		};
		uint32_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  virt_sob_id_t
 * \brief   virtual sob identifier for each engines
 * \details data structure to store virtual SOB IDs for each engine types
 */
struct virt_sob_id_t {
	uint16_t raw;
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  virt_sob_ids_t
 * \brief   virtual sob identifier for each engines
 * \details data structure to store virtual SOB IDs for each engine types
 */
struct virt_sob_ids_t {
	struct tpc_virt_sob_id_t tpc;
	/*
	 * Virtual SOB ID for TPC engines
	 */
	struct mme_virt_sob_id_t mme;
	/*
	 * Virtual SOB ID for MME engines
	 */
	struct mme_xpose_virt_sob_id_t mme_xpose;
	/*
	 * Virtual SOB ID for MME Transpose engines
	 */
	struct rot_virt_sob_id_t rot;
	/*
	 * Virtual SOB ID for ROTATOR engines
	 */
	struct dbg_virt_long_sob_id_t dbg;
	/*
	 * Virtual SOB ID for debug purpose
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \enum    rot_mcid_modes_t
 * \brief   Various MCID modes supported by Firmware
 * \details Various MCID modes supported by Firmware
 */
enum rot_mcid_modes_t {
	MCID_MODE_ABSOLUTE = 0x0,
	/**<
	 * Set MCID as absolute value, it can be
	 * used to set MCID as 0 as well
	 */
	MCID_MODE_RELATIVE_DISCARD = 0x1,
	/**<
	 * Set MCID relative to Discard MCID base
	 */
	MCID_MODE_RELATIVE_DEGRADE = 0x2,
	/**<
	 * Set MCID relative to Degrade MCID base
	 */
	MCID_MODE_RESERVED = 0x3,
	/**<
	 * Reserved for future use
	 */
	MCID_MODE_COUNT = 0x4
};

/**
 * \struct  rot_wd_ctxt_t
 * \brief   Rotator specific work distribution context
 * \details Rotator work distribution context for GC
 */
struct rot_wd_ctxt_t {
	union {
		uint32_t word0;
		struct {
			uint32_t switch_bit:1;
			/**
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t mcid_mode:2;
			/**
			 * Refer to rot_mcid_modes_t
			 */
			uint32_t cpl_msg_en:1;
			/**
			 * Enable Completion msg. Increment SOB only when this is set
			 */
			uint32_t reserved:4;
			/**
			 * reserved
			 */
			uint32_t sig_inc_value:16;
			/**
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:8;
			/**
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct virt_sob_ids_t virt_sob_ids;
	/**
	 * Virtual SOB array
	 */
	union {
		uint32_t mcids;
		struct {
			uint16_t rd_mcid_offset;
			/**<
			 * MCID Offset for read operation
			 */
			uint16_t wr_mcid_offset;
			/**<
			 * MCID Offset for write operation
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  rot_wd_ctxts_t
 * \brief   Rotator engine and sync scheme context
 * \details Rotator engine context and sync scheme context used for GC
 */
struct rot_wd_ctxts_t {
	struct rot_wd_ctxt_t rot_ctxt[WD_CTXT_COUNT];
	/**<
	 * array of contexts for Rotator
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

enum mme_op_type_t {
	MME_OP_COMPUTE = 0,
	MME_OP_TRANSPOSE = 1,
	MME_OP_COUNT = 2
};

/**
 * \struct  mme_wd_ctxt_t
 * \brief   MME specific work distribution context
 * \details MME work distribution context for GC
 */
struct mme_wd_ctxt_t {
	uint32_t mme_commit_reg;
	/**<
	 * mme commit register parameters
	 */

	union {
		uint32_t word0;
		struct {
			uint32_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t mme_op_type:4;
			/**<
			 * mme operation mode: COMPUTE/TRANSPOSE.
			 * Values are according to the enum - mme_op_type_t
			 * This is needed to -
			 *  1) Submit suitable WD Descriptor (with appropriate SOB ADDR Registers)
			 *  2) Increment the correct SOB according to the intended dependancy
			 */
			uint32_t reserved:3;
			/**<
			 * reserved
			 */
			uint32_t sig_inc_value:16;
			/**<
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:8;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  mme_wd_ctxts_t
 * \brief   MME engine and sync scheme context
 * \details MME engine context and sync scheme context used for GC
 */
struct mme_wd_ctxts_t {
	struct mme_wd_ctxt_t mme_ctxt[WD_CTXT_COUNT];
	/**<
	 * array of contexts for MME
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \enum    edma_op_type_t
 * \brief   Various EDMA operations
 * \details EDMA enums supported by Firmware
 *          Note: In EDMA_OP_TRANSPOSE and EDMA_OP_NO_WD, FW only does
 *                sync scheme
 */
enum edma_op_type_t {
	EDMA_OP_MEMSET_TENSOR = 0,
	EDMA_OP_MEMSET_LINEAR = 1,
	EDMA_OP_MEMCPY_TENSOR = 2,
	EDMA_OP_MEMCPY_LINEAR = 3,
	EDMA_OP_TRANSPOSE = 4, /* TODO: Remove this later */
	EDMA_OP_NO_WD = 5,
	EDMA_OP_COUNT = 6
};

/**<
 * Total number of EDMA engines involved in compute
 */
#define ENG_EDMA_COMPUTE_COUNT	5

/**
 * \struct  edma_tensor_t
 * \brief   EDMA tensor data structure
 * \details Data structure to store EDMA tensor related parameters
 */
struct edma_tensor_t {
	uint32_t grid_size[MAX_DIMENSIONS];
	/**<
	 * Size of the complete tensor
	 * DIM0 is in Bytes, rest all in elements
	 */
	uint32_t box_size[MAX_DIMENSIONS];
	/**<
	 * Chunk size of the tensor
	 * DIM0 is in Bytes, rest all in elements
	 * Note: Using box size field, firmware programs the tsize registers
	 * of source and destination tensors
	 */
	uint64_t addr_offset;
	/**<
	 * address offset of the main tensor,
	 * for the individual boxes the offsets would
	 * calculated by FW using this field
	 * Note: Firmware programs SRC and DST offset registers by using this
	 * field along with other parameters specified in this structure
	 */
	uint32_t box_offset[ENG_EDMA_COMPUTE_COUNT];
	/**<
	 * Each Engine processes only one box from the given grid.
	 * This field provides the offset that should be used by the
	 * engine to know which box it should process
	 * Firmware calculates the engine specific box offset as
	 * Engine Box offset = box_offset[Engine Index]
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_wd_ctxt_t
 * \brief   EDMA specific work distribution context
 * \details EDMA work distribution context for GC
 */
struct edma_wd_ctxt_t {
	uint32_t dma_commit_reg;
	union {
		uint32_t word0;
		struct {
			/**<
			 * dma commit register value to be written
			 */
			uint32_t dma_op:3;
			/**<
			 * DMA operation to be performed from edma_op_type_t
			 */
			uint32_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t shuffle_index:3;
			/**<
			 * Index of the 1st engine to start with, 3 bits
			 * Linear number starting from 0 to (num_engines - 1)
			 */
			uint32_t reserved:1;
			/**<
			 * reserved
			 */
			uint32_t sig_inc_value:16;
			/**<
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:8;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct edma_tensor_t dst_tensor;
	/**<
	 * Destination tensor configuration
	 */
	struct edma_tensor_t src_tensor;
	/**<
	 * Source tensor configuration
	 */
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_wd_ctxts_t
 * \brief   EDMA engine and sync scheme context
 * \details EDMA engine context and sync scheme context used for GC
 */
struct edma_wd_ctxts_t {
	struct edma_wd_ctxt_t edma_ctxt[WD_CTXT_COUNT];
	/**<
	 * array of contexts for EDMA
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  index_space_tensor_t
 * \brief   Data structure which defines index space tensor
 * \details Index space tensor parameters stored within GC Context
 */
struct index_space_tensor_t {
	uint32_t base_cord[MAX_DIMENSIONS];
	/**<
	 * Base coordinate of the grid
	 */
	uint32_t grid_size[MAX_DIMENSIONS];
	/**<
	 * Actual or Total Size of the index space tensor
	 */
	uint32_t box_size[MAX_DIMENSIONS];
	/**<
	 * Index space tensor is divided into small boxes
	 * Each box is processed by a particular TPC.
	 * Box size is nothing but amount of work
	 * that is processed by a TPC.
	 */
	uint32_t dim_slices[MAX_DIMENSIONS];
	/**<
	 * Number of slices in a dimension (Grid size/Box size div calculation)
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \enum    tpc_wd_mode_t
 * \brief   Various tpc wd modes supported by Firmware
 * \details Various tpc wd modes supported by Firmware
 */
enum tpc_wd_mode_t {
	TPC_WD_ST_MODE = 0x0,
	TPC_WD_SMT4_MODE = 0x1,
	TPC_WD_COUNT = 0x2
};

/**
 * \enum    tpc_wd_type_t
 * \brief   Various tpc wd types supported by Firmware
 * \details Various tpc wd types supported by Firmware
 */
enum tpc_wd_type_t {
	TPC_WD_USING_GLBL_INDEX = 0x0,
	TPC_WD_USING_DCORE_INDEX = 0x1,
	TPC_WD_TYPE_COUNT = 0x2
};

/**
 * \struct  tpc_wd_ctxt_t
 * \brief   TPC specific work distribution context
 * \details TPC specific work distribution context for GC
 */
struct tpc_wd_ctxt_t {
	struct index_space_tensor_t ist;
	/**<
	 * Index space tensor parameters
	 */
	union {
		uint32_t word1;
		struct {
			uint8_t shuffle_index;
			/**<
			 * Index of the 1st engine to start with, 5 bits
			 * Linear number starting from 0 to (num_engines - 1)
			 */
			uint8_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint8_t tpc_wd_mode:2;
			/**<
			 * Refer to tpc_wd_mode_t
			 */
			uint8_t wd_type:1;
			/**<
			 * Refer to tpc_wd_type_t
			 */
			uint8_t reserved:4;
			/**<
			 * Reserved
			 */
			uint8_t virtual_sob_bitmap;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
			uint8_t reserved2;
			/**<
			 * reserved
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	union {
		uint32_t word2;
		struct {
			uint16_t reserved1;
			/**<
			 * reserved
			 */
			uint16_t sig_inc_value;
			/**<
			 * Increment value to be added to previous threshold
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  tpc_wd_ctxts_t
 * \brief   TPC engine and sync scheme context
 * \details TPC engine context and sync scheme context used for GC
 */
struct tpc_wd_ctxts_t {
	struct tpc_wd_ctxt_t tpc_ctxt[WD_CTXT_COUNT];
	/**<
	 * Array of contexts for TPC
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_generic_t
 * \brief   generic command structure
 * \details Generic command structure
 */
struct eng_arc_cmd_generic_t {
	uint32_t cmd_type:4;
	/**<
	 * set to eng_arc_cmd_t
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start
	 */
	uint32_t reserved:24;
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  eng_arc_cmd_nop_t
 * \brief   NOP command structure
 * \details NOP command to align other commands on the buffer boundary
 */
struct eng_arc_cmd_nop_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_NOP
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ
	 * Engine FW pushes a NOP QMAN command into Static CQ or ARC CQ
	 * depending on the ECB list where this command is encountered
	 * and causes switch between Static CQ and ARC CQ
	 */
	uint32_t padding:23;
	/**<
	 * Number of DWORDS(4 Bytes) padded after this command.
	 * When padding = 0 means the size of DWORD command is 1 DWORD
	 */
} __attribute__ ((aligned(4), __packed__));

#define CPU_INDEX_ALL			0xFE
#define CPU_INDEX_INVALID		0xFF

/**
 * \struct  eng_arc_cmd_static_desc_v2_t
 * \brief   Static CP DMA transfer
 * \details Push a descriptor into static CQ. The content of the descriptor
 *	    is not known to Firmware. The descriptor buffer can contain
 *	    any valid QMAN commands
 */
struct eng_arc_cmd_static_desc_v2_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_STATIC_DESC_V2
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t reserved:3;
	/**<
	 * Reserved, unused
	 */
	uint32_t cpu_index:8;
	/**<
	 * Virtual CPU Index from 0 to N-1, where N is number of engines
	 *
	 * cpu_index = CPU_INDEX_ALL command is processed by all engine ARCs
	 * cpu_index = CPU_INDEX_INVALID command is ignored by all engine ARCs
	 */
	uint32_t size:13;
	/**<
	 * transfer size in bytes
	 * TODO: can be coverted to DWORD if 21bits are not enough
	 */
	uint32_t addr_index:3;
	/**<
	 * Recipe base address register index to be used to generate target
	 * address of 64 bits
	 */
	uint32_t addr_offset;
	/**<
	 * 32bit address offset
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_wd_fence_and_exec_t
 * \brief   Work distribution, fence and execute
 * \details Execute Command for Compute Engines
 */
struct eng_arc_cmd_wd_fence_and_exec_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_WD_FENCE_AND_EXE
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start.
	 * Expected value is 1.
	 */
	uint32_t reserved:19;
	/**<
	 * reserved
	 */
	uint32_t wd_ctxt_id:3;
	/**<
	 * a context number from 0 to max number of contexts that fw supports
	 */
	uint32_t reserved2:2;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_sched_dma_t
 * \brief   Schedule DMA to update GC context
 * \details Initiate a DMA transfer to update GC context. Any command which
 *	    depends on this transfer to be completed can use the dma_completion
 *	    field of that command to wait until GC context is updated.
 */
struct eng_arc_cmd_sched_dma_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_SCHED_DMA
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t addr_index:3;
	/**<
	 * Recipe base address register index to be used to generate target
	 * address of 64 bits
	 */
	uint32_t size:10;
	/**<
	 * size of the buffer in bytes
	 */
	uint32_t wd_type:1;
	/**<
	 * Refer to tpc_wd_type_t
	 */
	uint32_t gc_ctxt_offset:13;
	/*
	 * destination address where the DMA needs to be done
	 * offset of the location within GC managed struct in the DCCM
	 */
	uint32_t addr_offset;
	/**<
	 * 32bit address offset into recipe base address
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_sfg_t
 * \brief   Signal From Graph
 * \details Signals to a particular SOB outside of the graph sync scheme.
 *	    This can be used to synchronize compute stream with network stream
 *	    without waiting for the completion of entire graph.
 */
struct eng_arc_cmd_sfg_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_SFG
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ bit is used by firmware to set the switch_bit
	 * in the last QMAN command to switch the CQ
	 */
	uint32_t sob_inc_value_xpose:11;
	/**<
	 * The value used to increment MME xpose SOB
	 */
	uint32_t sob_inc_value:15;
	/**<
	 * The value that should be used to increment the SOB
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_reset_soset_t
 * \brief   Reset SO Set
 * \details Reset active sync object set when one of the SOs overflows.
 */
struct eng_arc_cmd_reset_soset_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_RESET_SOSET
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ bit is used by firmware to set the switch_bit
	 * in the last QMAN command to switch the CQ
	 */
	uint32_t num_cmpt_engines:10;
	/**<
	 * Total no. of active physical engines including CME
	 */
	uint32_t target:16;
	/**<
	 * Based on the engine type this value is either of the following
	 * struct mme_virt_sob_id_t
	 * struct tpc_virt_sob_id_t
	 * struct rot_virt_sob_id_t
	 */
	uint32_t target_xpose:16;
	/**<
	 * GC needs to set this value only for mme engine
	 */
	uint32_t reserved:16;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_mcid_rollover_t
 * \brief   MCID Rollover
 * \details Handle MCID Rollover within graph.
 */
struct eng_arc_cmd_mcid_rollover_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_MCID_ROLLOVER
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ bit is used by firmware to set the switch_bit
	 * in the last QMAN command to switch the CQ
	 */
	uint32_t :10;
	/**<
	 * unused
	 */
	uint32_t target:16;
	/**<
	 * Based on the engine type this value is either of the following
	 * struct mme_virt_sob_id_t
	 * struct tpc_virt_sob_id_t
	 * struct rot_virt_sob_id_t
	 */
	uint32_t target_xpose:16;
	/**<
	 * GC needs to set this value only for mme engine
	 */
	uint32_t :16;
	/**<
	 * unused
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_list_size_t
 * \brief   Schedule DMA to update GC context
 * \details This is the first command in the chunk to indicate size of the list
 *	    Its present in static and dynamic list both.
 */
struct eng_arc_cmd_list_size_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_LIST_SIZE
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t topology_start:1;
	/**<
	 * start of new topology, fw can reset
	 * prev_sob_id etc. when this flag is set to 1
	 */
	uint32_t reserved:2;
	/**<
	 * reserved
	 */
	uint32_t list_size:24;
	/**<
	 * Total size of list in bytes; for FW management of double buffer
	 * The size includes the size of this command as well.
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  cme_arc_cmd_discard_cls_t
 * \brief   Discard Cache Lines
 * \details Cache maintanance command to discard cache lines
 */
struct cme_arc_cmd_discard_cls_t {
	uint32_t cmd_type:4;
	/**<
	 * set to CME_ARC_CMD_DISCARD_CLS
	 */
	uint32_t cls:2;
	/**<
	 * Class that will be forced to matching CL, MAINT_ATTR_MCID
	 */
	uint32_t notify_profiler:1;
	/**<
	 * Send a notification to Profiler when Discard event is writen to DUP
	 * This is implemented using an additional monitor pair and will give
	 * the time at which all discard depenancies are met and FW submits a
	 * CS DISCARD command to the DUP. The DUP will further broadcast this to
	 * all CS and hence this time is only an approximate value.
	 */
	uint32_t reserved:5;
	/**<
	 * Reserved
	 */
	uint32_t target_bitmap:4;
	/**<
	 * Bitmap indicating index which are valid
	 * Refer to enum sched_cmpt_sync_scheme_bitmap
	 */
	uint32_t mcid_offset:16;
	/**<
	 * MCID offset, relative to the discard mcid base id
	 */
	struct tpc_virt_sob_id_t tpc;
	/*
	 * SOB Target for TPC engines
	 */
	struct mme_virt_sob_id_t mme;
	/*
	 * SOB Target for MME engines
	 */
	struct mme_xpose_virt_sob_id_t mme_xpose;
	/*
	 * SOB Target for MME Transpose engines
	 */
	struct rot_virt_sob_id_t rot;
	/*
	 * SOB Target for ROTATOR engines
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  cme_arc_cmd_degrade_cls_t
 * \brief   Degrade Cache Lines
 * \details Cache maintanance command to degrade cache lines
 */
struct cme_arc_cmd_degrade_cls_t {
	uint32_t cmd_type:4;
	/**<
	 * set to CME_ARC_CMD_DEGRADE_CLS
	 */
	uint32_t cls:2;
	/**<
	 * Class that will be forced to matching CL, MAINT_ATTR_MCID
	 */
	uint32_t reserved:5;
	/**<
	 * Reserved
	 */
	uint32_t use_discard_base:1;
	/**<
	 * Special scenario where in the degrade command the firmware is
	 * expected to use discard base.
	 */
	uint32_t target_bitmap:4;
	/**<
	 * Bitmap indicating index which are valid
	 * Refer to enum sched_cmpt_sync_scheme_bitmap
	 */
	uint32_t mcid_offset:16;
	/**<
	 * MCID offset, relative to the discard mcid base id
	 */
	struct tpc_virt_sob_id_t tpc;
	/*
	 * SOB Target for TPC engines
	 */
	struct mme_virt_sob_id_t mme;
	/*
	 * SOB Target for MME engines
	 */
	struct mme_xpose_virt_sob_id_t mme_xpose;
	/*
	 * SOB Target for MME Transpose engines
	 */
	struct rot_virt_sob_id_t rot;
	/*
	 * SOB Target for ROTATOR engines
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  cme_arc_cmd_reset_soset_t
 * \brief   Reset SO Set within the ECB List
 * \details Command to reset SO set within the ECB List
 */
struct cme_arc_cmd_reset_soset_t {
	uint32_t cmd_type:4;
	/**<
	 * set to CME_ARC_CMD_RESET_SOSET
	 */
	uint32_t num_engines:10;
	/**<
	 * Total no. of active physical engines including CME
	 */
	uint32_t reserved:18;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  cme_arc_cmd_mcid_rollover_t
 * \brief   Discard MCID rollover within the ECB List
 * \details Command to handle Discard MCID roll over within the ECB List
 */
struct cme_arc_cmd_mcid_rollover_t {
	uint32_t cmd_type:4;
	/**<
	 * set to CME_ARC_CMD_MCID_ROLLOVER
	 */
	uint32_t signal_mme:1;
	/**<
	 * When MMEs are also involved in MCID Rollover handling
	 * this flag needs to be one, so that CME can signal
	 * it to resume work
	 */
	uint32_t signal_rot:1;
	/**<
	 * When Rotators are also involved in MCID Rollover handling
	 * this flag needs to be one, so that CME can signal
	 * it to resume work
	 */
	uint32_t reserved:26;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  cme_arc_cmd_nop_t
 * \brief   NOP command structure
 * \details NOP command to align other commands on the buffer boundary
 */
struct cme_arc_cmd_nop_t {
	uint32_t cmd_type:4;
	/**<
	 * set to CME_ECBL_CMD_NOP
	 */
	uint32_t padding:28;
	/**<
	 * Number of DWORDS(4 Bytes) padded after this command.
	 * When padding = 0 means the size of DWORD command is 1 DWORD
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * Total number of NIC collective contexts stored in Fimrware
 * 0 to 7 are RECV context
 * 8 to 15 are SEND context
 */
enum {
	SCALEUP_NIC_COLL_CTXT_COUNT = 16,

	SCALEOUT_NIC_COLL_CTXT_COUNT = 5
};

/**
 * NIC collective ops strategy to be used
 */
enum nic_coll_strategy_t {
	NIC_STRATEGY_ZERO = 0,
	/**<
	 * offset += 0.
	 */
	NIC_STRATEGY_REMOTE_RANK = 1,
	/**<
	 * offset += stride * remote_index
	 */
	NIC_STRATEGY_LOCAL_RANK = 2
	/**<
	 * offset += stride * local_index
	 */
};

/**
 * \enum    eng_arc_cmd_t
 * \brief   Various engine commands
 * \details Command type enum specifies the commands that are processed by
 *	    compute engines ARCs.
 */
enum edma_eng_arc_cmd_t {
	NIC_EDMA_CMD_SIBO_OPS_V3 = 0,
	NIC_EDMA_CMD_LIN_OPS_V3 = 1,
	NIC_EDMA_CMD_SIBO_MEMSET_V3 = 2,
	NIC_EDMA_CMD_LIN_MEMSET_V3 = 3,
	NIC_EDMA_CMD_UNUSED4 = 4,
	NIC_EDMA_CMD_CAST_DOWN_CLEAR = 5,
	NIC_EDMA_CMD_SIBO_MEMSET_V3_2 = 6,
	NIC_EDMA_CMD_MEMCPY_V3 = 7,
	NIC_EDMA_CMD_UPDATE_GLBL_CTXT_V3 = 8,
	NIC_EDMA_CMD_CAST_UP_BATCH_V3 = 9,
	NIC_EDMA_CMD_COUNT = 10,
	NIC_EDMA_COUNT = NIC_EDMA_CMD_COUNT /* Depricated, dont use */
};

/**
 * \struct  arc_cmd_nic_edma_ops_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations associated with
 *	    EDMAs associated with NICs.
 */
struct arc_cmd_nic_edma_ops_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t fp16:1;
		/**<
		 * valid only for cast down operation. Ignored otherwise.
		 * When set to 1, cast to fp16. otherwise cast to bf16.
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t shuffle_index:3;
		/**<
		 * Shuffle Index
		 */
		uint32_t use_sibo_index_as_src:1;
		/**<
		 * Use SIB Order buffer indexes instead of addresses
		 */
		uint32_t sibo_index:12;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:4;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t hbw_axcache:8;
		/**<
		 * HBW AX CACHE Setting for READ/WRITE.
		 * RD[3:0]
		 * WR[7:4]
		 * If no value is supplied, FW will configure to 0x33 (RESET Value)
		 */
		uint32_t memset:1;
		/**<
		 * memset
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low
	 * Note: In case of the memset this field contains value
	 * that needs to be used for memset operation
	 */
	union {
		struct {
			uint32_t src_addr_hi:24;
			/**<
			 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
			 * Note: In case of memset operation this field is ignored
			 */
			uint32_t reduction_ind:1;
			/**<
			 * Reduction indication
			 */
			uint32_t pool_id:1;
			/**<
			 * pool id to select which intermediate buffer to be used
			 */
			uint32_t reduction_op:2;
			/**<
			 * Reduction operation to be performed
			 */
			uint32_t reduction_dtype:4;
			/**<
			 * Reduction data type
			 * 0xC- upscaling FP16
			 * 0xD- upscaling BF16
			 */
		} __attribute__ ((aligned(4), __packed__));
		struct {
			uint32_t reserved_1:24;
			/**<
			 * reserved
			 */
			uint32_t reduction_7_0:8;
			/**<
			 * Reduction bits
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_OPS_V3_SIZE (sizeof(struct arc_cmd_nic_edma_ops_v3_t))
#define ARC_CMD_NIC_EDMA_OPS_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_OPS_V3_SIZE / 4)

/**
 * \enum    cache_cls_type_e
 * \brief   Cache class type
 * \details Class is a 2-bit field that directs cache replacement logic
 */
enum cache_cls_type_e {
	CACHE_CLS_LOW = 0,
	CACHE_CLS_NORMAL = 1,
	CACHE_CLS_HIGH = 2,
	CACHE_CLS_TOP = 3
};

/**
 * \enum    cache_allocation_type_e
 * \brief   AxCache allocation hint
 * \details Allocation hint is a 4-bit value that directs HW if to place the
 *			desired cache line in cache and if so - in which portion of the cache it
 *			should be placed
 */
enum cache_allocation_type_e {
	/* No allocation will be performed into cache */
	CACHE_ALLOC_NO = 0x3,
	/* Data will be allocated in the Home cache */
	CACHE_ALLOC_H = 0x7,
	/* Data will be allocated in DCORE cache and removed from remote caches */
	CACHE_ALLOC_D = 0xB,
	/* Data will be allocated in DCORE cache and in Home Cache (i.e., replicated).
	 * Evictions of each (D$, H$) will be done separately
	 */
	CACHE_ALLOC_DH = 0xF
};

/**
 * \enum    nic_edma_datasizes_t
 * \brief   edma operation data type
 * \details Data type for edma operations, local datatype, output datatype
 */
enum nic_edma_datasizes_t {
	NIC_EDMA_8BITS = 0x0,
	NIC_EDMA_16BITS = 0x1,
	NIC_EDMA_32BITS = 0x2,
	NIC_EDMA_DTYPE_MAX = 0x3
};

/**
 * \enum    nic_edma_datatypes_t
 * \brief   edma operation data type
 * \details Data type for edma operations, local datatype, output datatype
 */
enum nic_edma_datatypes_t {
	NIC_EDMA_UNSIGNED = 0x0,
	NIC_EDMA_SIGNED = 0x1,
	NIC_EDMA_FP = 0x2,
	NIC_EDMA_BF = 0x3
};

/**
 * \struct  arc_cmd_nic_edma_sibo_ops_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_sibo_ops_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_base:3;
		/**<
		 * index of SOB base in glbl ctxt that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t sob_index:10;
		/**<
		 * SOB index from comp_cfg base that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t signal_second:1;
		/**<
		 * Whether second_signal is needed or not
		 */
		uint32_t second_sob_base:3;
		/**<
		 * index of SOB base in glbl ctxt that needs to be used for signaling second
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t second_sob_index:10;
		/**<
		 * SOB index from comp_cfg base that needs to be used for signaling second
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :1;
		/**<
		 * unused
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:3;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t local_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t sibo_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t output_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t local_hbw_axcache:4;
		/**<
		 * HBW AX CACHE Setting for read operation.
		 * RD[0:3]
		 */
		uint32_t local_class_type:2;
		/**<
		 * HB CLASS TYPE Setting for read operation
		 * RD[4:5]
		 */
		uint32_t output_hbw_axcache:4;
		/**<
		 * HBW AX CACHE Setting for write operation.
		 * WR[0:1]
		 */
		uint32_t output_class_type:2;
		/**<
		 * HB CLASS TYPE Setting for write operation
		 * WR[0:1]
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t transfer_size:23;
		/**<
		 * transfer size in bytes
		 */
		uint32_t sibo_index:9;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
	} __attribute__ ((aligned(4), __packed__));
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low for the local buffer
	 */
	union {
		struct {
			uint32_t src_addr_hi:24;
			/**<
			 * Source address hi for the local buffer
			 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
			 */
			uint32_t reduction_ind:1;
			/**<
			 * Reduction indication
			 */
			uint32_t reduction_in_place:1;
			/**<
			 * TODO: This flag is kept for future use.
			 * We haven't received clear requirements yet, but
			 * keeping this flag so that we dont forget
			 */
			uint32_t wide_accumulation:1;
			/**<
			 * When set reduction operation should happen in FP32 data type.
			 * All the inputs should be up casted to FP32 and copied
			 * to output after appropriate conversion based on output
			 * data type
			 */
			uint32_t reduction_op:3;
			/**<
			 * Reduction operation to be performed
			 */
			uint32_t dtype:2;
			/**<
			 * Type of data, which includes unsigned, singed, FP, and BF.
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE (sizeof(struct arc_cmd_nic_edma_sibo_ops_v3_t))
#define ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_lin_ops_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_lin_ops_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t :2;
		/**<
		 * unused
		 */
		uint32_t hbw_axcache:8;
		/**<
		 * HBW AX CACHE Setting for READ/WRITE.
		 * RD[0:3]
		 * WR[4:7]
		 */
		uint32_t class_type:6;
		/**<
		 * HB CLASS TYPE Setting
		 * WR[0:1]
		 * RD[4:5]
		 * Bits[2:3] don't care
		 */
		uint32_t input_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t output_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t dtype:2;
		/**<
		 * Type of data, which includes unsigned, signed, FP, and BF.
		 * 0 -> unsigned
		 * 1 -> signed
		 * 2 -> FP16
		 * 3 -> BF16
		 * refer to nic_edma_datatypes_t
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :3;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low for the local buffer
	 */
	union {
		struct {
			uint32_t src_addr_hi:24;
			/**<
			 * Source address hi for the local buffer
			 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
			 * TODO: Can we reduce this to 8 Bits ? 96GB of Cache
			 */
			uint32_t reduction_ind:1;
			/**<
			 * Reduction indication
			 */
			uint32_t reduction_in_place:1;
			/**<
			 * TODO: This flag is kept for future use.
			 * We haven't received clear requirements yet, but
			 * keeping this flag so that we dont forget
			 */
			uint32_t reduction_op:3;
			/**<
			 * Reduction operation to be performed
			 */
			uint32_t :3;
			/**<
			 * unused
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE (sizeof(struct arc_cmd_nic_edma_lin_ops_v3_t))
#define ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_sibo_memset_v3_t
 * \brief   Perform Memset operation on SIBO buffer
 * \details Perform Memset operation on SIBO buffer
 *	    associated with NICs. HCL team uses this to clear buffers
 *          partially in SIBO , thats why we cant just use linear memset
 *          instead of this.
 */
struct arc_cmd_nic_edma_sibo_memset_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
		/**<
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t :2;
		/**<
		 * unused
		 */
		uint32_t sibo_index:9;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:3;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t hbw_axcache:4;
		/**<
		 * HBW AX CACHE Setting for write operation.
		 * WR[4:7]
		 */
		uint32_t class_type:2;
		/**<
		 * HB CLASS TYPE Setting for write operation
		 * WR[0:1]
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :1;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t memset_value;
	/**<
	 * Value to be used for memset
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE (sizeof(struct arc_cmd_nic_edma_sibo_memset_v3_t))
#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_sibo_memset_v3_2_t
 * \brief   Perform Memset operation on SIBO buffer
 * \details Perform Memset operation on SIBO buffer
 *	    associated with NICs. HCL team uses this to clear buffers
 *          partially in SIBO, thats why we cant just use linear memset
 *          instead of this.
 */
struct arc_cmd_nic_edma_sibo_memset_v3_2_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
		/**<
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t sibo_index:9;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:4;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t hbw_axcache:4;
		/**<
		 * HBW AX CACHE Setting for write operation.
		 * WR[4:7]
		 */
		uint32_t class_type:2;
		/**<
		 * HB CLASS TYPE Setting for write operation
		 * WR[0:1]
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :2;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t memset_value;
	/**<
	 * Value to be used for memset
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_2_SIZE (sizeof(struct arc_cmd_nic_edma_sibo_memset_v3_2_t))
#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_2_SIZE_DWORD (ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_2_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_lin_memset_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_lin_memset_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t :2;
		/**<
		 * unused
		 */
		uint32_t hbw_axcache:4;
		/**<
		 * HBW AX CACHE Setting for write operation
		 * WR[4:7]
		 */
		uint32_t class_type:2;
		/**<
		 * HB CLASS TYPE Setting for write operation
		 * WR[0:1]
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :17;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t memset_value;
	/**<
	 * Value to be used for memset
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_LIN_MEMSET_V3_SIZE (sizeof(struct arc_cmd_nic_edma_lin_memset_v3_t))
#define ARC_CMD_NIC_EDMA_LIN_MEMSET_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_LIN_MEMSET_V3_SIZE / 4)

#define FP32_MAX_POS_VAL	0x7F800000
#define FP32_MAX_NEG_VAL	0xFF800000

enum cdc_op_type_t {
	CDC_OP_CLEAR = 0,
	CDC_OP_SET_MAX_INF = 1, /* 0x7F80_0000 */
	CDC_OP_UNSUPPORTED = 2,
	CDC_OP_SET_MIN_INF = 3 /* 0xFF80_0000 */
};

/**
 * \struct  arc_cmd_nic_edma_ops_cdc_t
 * \brief   Perform Cast Down Clear EDMA operation
 * \details Perform CDC EDMA operation with dual signalling capability
 */
struct arc_cmd_nic_edma_ops_cdc_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling 1st completion
		 * address = 0xF8000000 | sob_address;
		 *
		 * If this value is 0, then Cast-down will not be performed and
		 * only the Mem Clear of SRC buffers will be done
		 */
		uint32_t fp16:1;
		/**<
		 * valid only for cast down operation. Ignored otherwise.
		 * When set to 1, cast to fp16. otherwise cast to bf16.
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t sob_address2;
	/**<
	 * 2nd SOB address that needs to be used for signaling 2nd completion
	 * address = 0xF8000000 | sob_address;
	 */
	struct {
		uint32_t shuffle_index:3;
		/**<
		 * Shuffle Index
		 */
		uint32_t use_sibo_index_as_src:1;
		/**<
		 * Use SIB Order buffer indexes instead of addresses
		 */
		uint32_t sibo_index:6;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:4;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t memset_op:2;
		/**<
		 * Values are according to enum cdc_op_type
		 * 0: Clear the SRC Memory
		 * 1: memset with Positive MAX (0x7F80_0000)
		 * 2: UNSUPPORTED
		 * 3: memset with Negative MAX (0xFF80_0000)
		 */
		uint32_t hbw_axcache:8;
		/**<
		 * HBW AX CACHE Setting for READ/WRITE.
		 * RD[3:0]
		 * WR[7:4]
		 * If no value is supplied, FW will configure to 0x33 (RESET Value)
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
		uint32_t :4;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low
	 * Note: In case of the memset this field contains value
	 * that needs to be used for memset operation
	 */
	union {
		struct {
			uint32_t src_addr_hi:24;
			/**<
			 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
			 * Note: In case of memset operation this field is ignored
			 */
			uint32_t reduction_ind:1;
			/**<
			 * Reduction indication
			 */
			uint32_t reserved_0:1;
			/**<
			 * Reserved
			 */
			uint32_t reduction_op:2;
			/**<
			 * Reduction operation to be performed
			 */
			uint32_t reduction_dtype:4;
			/**<
			 * Reduction data type
			 * 0xC- upscaling FP16
			 * 0xD- upscaling BF16
			 */
		} __attribute__ ((aligned(4), __packed__));
		struct {
			uint32_t reserved_1:24;
			/**<
			 * reserved
			 */
			uint32_t reduction_7_0:8;
			/**<
			 * Reduction bits
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_OPS_CDC_SIZE (sizeof(struct arc_cmd_nic_edma_ops_cdc_t))
#define ARC_CMD_NIC_EDMA_OPS_CDC_SIZE_DWORD (ARC_CMD_NIC_EDMA_OPS_CDC_SIZE / 4)

/**
 * EDMA NIC maximum dwords in glbl ctxt
 */
enum {
	EDMA_NIC_SIBO_BASE_COUNT = 2,
	EDMA_NIC_MAX_DWORDS_IN_CTXT = 6,
	EDMA_NIC_COMP_CFG_MAX = 8,
	EDMA_NIC_MAX_DWORDS_IN_CTXT_V2 = 9,
	EDMA_NIC_MAX_DWORDS_IN_CTXT_V3 = 17
};

/*
 * DWORD position of different members of EDMA glbl ctxt
 */
enum {
	EDMA_NIC_SIB_BASE_ADDR_LO_0 = 0,
	EDMA_NIC_SIB_BASE_ADDR_HI_0 = 1,
	EDMA_NIC_SIB_BASE_ADDR_LO_1 = 2,
	EDMA_NIC_SIB_BASE_ADDR_HI_1 = 3,
	EDMA_NIC_SIB_RANK_STRIDE_0 = 4,
	EDMA_NIC_SIB_RANK_STRIDE_1 = 5,
	EDMA_NIC_SIRB_BASE_ADDR_LO = 6,
	EDMA_NIC_SIRB_BASE_ADDR_HI = 7,
	EDMA_NIC_SIRB_SIZE = 8,
	EDMA_NIC_COMP_CFG_0 = 9,
	EDMA_NIC_COMP_CFG_1 = 10,
	EDMA_NIC_COMP_CFG_2 = 11,
	EDMA_NIC_COMP_CFG_3 = 12,
	EDMA_NIC_COMP_CFG_4 = 13,
	EDMA_NIC_COMP_CFG_5 = 14,
	EDMA_NIC_COMP_CFG_6 = 15,
	EDMA_NIC_COMP_CFG_7 = 16,
	EDMA_NIC_UPDATE_BITMAP_SIZE  = EDMA_NIC_MAX_DWORDS_IN_CTXT_V3
};

/**
 * \struct  edma_nic_glbl_ctxt_v3_t
 * \brief   EDMA NIC global context
 * \details This data structure contains information related to buffers
 *	    which are required by various EDMA cmds
 */
struct edma_nic_glbl_ctxt_v3_t {
	union {
		struct {
			uint64_t sib_base_addr[EDMA_NIC_SIBO_BASE_COUNT];
			/**<
			 * Address array of Static Intermediate Buffer for storing incoming
			 * data in order, base addr to be selected on the basis of pool_id
			 * Actual Address: sib_order_base_addr + (sibo_rank_stride * 8) * index
			 */
			uint32_t sibo_rank_stride[EDMA_NIC_SIBO_BASE_COUNT];
			/**<
			 * Array of sib_order Rank stride, in other words maximum size of
			 * sib_order sub buffers for each rank
			 */
			uint64_t sirb_base_addr;
			/**<
			 * Base address of Static Intermediate reduction Buffer (sirb) to store
			 * reduction data in order
			 */
			uint32_t sirb_size;
			/**<
			 * Total size of intermediate reduction buffer (sirb size)
			 */
			uint32_t comp_cfg[EDMA_NIC_COMP_CFG_MAX];
			/**<
			 * Completion group cfg base which will be used to derive the completion
			 * SOB base address
			 */
		};
		uint32_t raw_dwords[EDMA_NIC_MAX_DWORDS_IN_CTXT_V3];
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_nic_wd_ctxts_t
 * \brief   EDMA NIC engine WD context
 * \details EDMA engine context used for WD
 */
struct edma_nic_wd_ctxts_t {
	/**<
	 * contexts for NIC EDMA.
	 */
	struct edma_nic_glbl_ctxt_v3_t edma_nic_ctxt_v3;
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  arc_cmd_update_edma_nic_ctxt_v3_t
 * \brief   Update the ctxt for NIC-EDMAs
 * \details Update the ctxt for NIC-EDMAs
 */
struct arc_cmd_update_edma_nic_ctxt_v3_t {
	uint32_t opcode:4;
	/**<
	 * opcode of the command = NIC_EDMA_CMD_UPDATE_GLBL_CTXT_V3
	 */
	uint32_t update_bitmap:18;
	/**<
	 * Bitmap of the DWORDs, which needs to be updated
	 */
	uint32_t reserved:5;
	/**<
	 * reserved
	 */
	uint32_t num_dwords:5;
	/**<
	 * Number of DWORDS to be updated by this command
	 */
	struct {
		uint32_t sob_address:28;
		/**<
		 * SOB address that needs to be used for signaling completion
		 *
		 * If this value is 0, then Signalling will not happen
		 */
		uint32_t rsvd:4;
		/**<
		 * reserved
		 */
	} __attribute__ ((aligned(4), __packed__));
	uint32_t data[0];
	/**<
	 * Actual DWORDS data to be updated by this command
	 */
} __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE (sizeof(struct arc_cmd_update_edma_nic_ctxt_v3_t))
#define ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE_DWORD (ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE / 4)
#define ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE (sizeof(struct arc_cmd_update_edma_nic_ctxt_v3_t) +	\
	(EDMA_NIC_MAX_DWORDS_IN_CTXT_V3 * sizeof(uint32_t)))
#define ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE_DWORD (ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE / 4)

#endif /* __GAUDI3_ARC_ENG_PACKETS_H__ */
