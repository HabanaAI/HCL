/* SPDX-License-Identifier: MIT
 *
 * Copyright (C) 2020 HabanaLabs Ltd.
 * All Rights Reserved.
 */

#ifndef __GAUDI2_ARC_ENG_PACKETS_H__
#define __GAUDI2_ARC_ENG_PACKETS_H__

#include <stdint.h>
#include "gaudi2_arc_common_packets.h"

/**
 * \enum    eng_job_id_t
 * \brief   Ids of commonly used kernels.
 * \details  Ids of commonly used kernels.
 */
enum eng_job_id_t {
	/**<
	 * This is the id of a barrier job.
	 */
	ENG_BARRIER_JOB_ID = 0xE000,

	/**<
	 * This is the id of a NULL job.
	 * This is submitted when a specific engine does not
	 * have a valid work to do as part of Work distribution
	 * (or) SFG (or) MCID Rollover
	 */
	ENG_NULL_JOB_ID = 0xFFFF
};

/**
 * \enum    eng_arc_cmd_t
 * \brief   Various engine commands
 * \details Command type enum specifies the commands that are processed by
 *	    compute engines ARCs. These are part of ECB list.
 */
enum eng_arc_cmd_t {
	ECB_CMD_LIST_SIZE = 0,
	ECB_CMD_NOP = 1,
	ECB_CMD_WD_FENCE_AND_EXE = 2,
	ECB_CMD_SCHED_DMA = 3,
	ECB_CMD_SCHED_DMA_V2 = 4,
	ECB_CMD_STATIC_DESC_V2 = 5,
	ECB_CMD_SFG = 6,
	ECB_CMD_RESET_SOSET = 7,
	ECB_CMD_COUNT = 8
};

/**
 * \enum    eng_compute_arc_cmd_t
 * \brief   Various opcodes for all compute engine arcs
 * \details Command IDs for commands sent to Compute Engine ARC from Scheduler ARC
 *	    These opcodes are internally generated by scheduler. Used by profiler
 *	    to display traces.
 *	    These enums are for internal usage of scheduler firmware and
 *	    kept here to prepare profiler event table. This should not be
 *	    used by any other components other than ARC firmware.
 */
enum eng_compute_arc_cmd_t {
	ENG_COMPUTE_ARC_CMD_BARRIER = 0x0,
	ENG_COMPUTE_ARC_CMD_ALLOC_BARRIER = 0x1,
	ENG_COMPUTE_ARC_CMD_DISPATCH_COMPUTE_ECB_LIST_V3 = 0x2,
	ENG_COMPUTE_ARC_CMD_UPDATE_RECIPE_BASE_V2 = 0x3,
	ENG_COMPUTE_ARC_CMD_WAIT_FOR_EXT_SIGNAL = 0x4,
	ENG_COMPUTE_ARC_CMD_COUNT = 0x5,
	ENG_COMPUTE_ARC_CMD_SIZE = 0x1F
};

/**
 * \enum    eng_pdma_arc_cmd_t
 * \brief   Various opcodes for all PDMA RX engine arc
 * \details Command IDs for commands sent to PDMA Engine ARC from Scheduler ARC
 *	    These opcodes are internally generated by scheduler. Used by profiler
 *	    to display traces.
 *	    These enums are for internal usage of scheduler firmware and
 *	    kept here to prepare profiler event table. This should not be
 *	    used by any other components other than ARC firmware.
 */
enum eng_pdma_arc_cmd_t {
	ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION = 0x0,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode.
	 * Firmware splits them into smaller chunk and submits that to PDMA engine
	 * Arbitration happens on the chunk boundary, so higher prority requests
	 * can preempt the lower priority transfers
	 */
	ENG_PDMA_ARC_CMD_BATCH_NO_FRAGMENTATION = 0x1,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode.
	 * Firmware does not split them into smaller chunk and submits that to PDMA engine
	 * as single transfer. Once submitted this transfer will run to completion.
	 */
	ENG_PDMA_ARC_CMD_UNUSED2 = 2,
	/**<
	 * unused opcode, kept to keep CI healthy
	 */
	ENG_PDMA_ARC_CMD_UNUSED3 = 3,
	/**<
	 * unused opcode, kept to keep CI healthy
	 */
	ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION_CASTUP = 0x4,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode. This command
	 * supports cast up functionality along with optional reduction support.
	 * Firmware splits them into smaller chunks and submit them to PDMA engine
	 * Arbitration happens on the chunk boundary, so higher prority requests
	 * can preempt the lower priority transfers at the chunk boundary.
	 */
	ENG_PDMA_ARC_CMD_BATCH_NO_FRAGMENTATION_CASTUP = 0x5,
	/**<
	 * User can submit a batch of PDMA transfers using this opcode. This command
	 * supports cast up functionality along with optional reduction support.
	 * Firmware does not split them into smaller chunks and submits single transfer request
	 * to PDMA engine. So once submitted it runs till completion as the arbitration happens
	 * only at the end of transfer.
	 */
	ENG_PDMA_ARC_CMD_BATCH_USER_DATA = ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION,
	/**<
	 */
	ENG_PDMA_ARC_CMD_BATCH_COMMANDS = ENG_PDMA_ARC_CMD_BATCH_NO_FRAGMENTATION,
	/**<
	 */
	ENG_PDMA_ARC_CMD_RX_USER_DATA = ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION,
	/**<
	 *
	 */
	ENG_PDMA_ARC_CMD_TX_NETWORK_MEMOPS = ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION,
	/**<
	 * Network applications should use this command to perform Memset/Memcopy
	 * operations with optional reduction support for Host to Device transfers
	 */
	ENG_PDMA_ARC_CMD_TX_NETWORK_CASTUP = ENG_PDMA_ARC_CMD_BATCH_WITH_FRAGMENTATION_CASTUP,
	/**<
	 * Network applications should use this command to perform Cast Up
	 * operations with Host to Device transfers. Reduction parameters must be
	 * set while doing cast up
	 */
	ENG_PDMA_ARC_CMD_RX_NETWORK_MEMOPS = ENG_PDMA_ARC_CMD_BATCH_NO_FRAGMENTATION,
	/**<
	 * Network applications should use this command to perform Memset/Memcopy
	 * operations for Device to Host and Device to Device transfers.
	 * Reduction wont work for Rx transfers as the Host memory controller
	 * does not support reduction
	 */
	ENG_PDMA_ARC_CMD_COUNT = 0x6,
	/**<
	 * Total number of PDMA commands
	 */
	ENG_PDMA_ARC_CMD_SIZE = 0x1F
};

/**
 * \enum    nic_eng_arc_cmd_t
 * \brief   Various NIC commands
 * \details Command type enum specifies the commands that are processed by
 *	    NIC engines ARCs.
 */
enum nic_eng_arc_cmd_t {
	NIC_CMD_UPDATE_GLBL_CTXT = 0,
	NIC_CMD_UPDATE_COLL_CTXT = 1,
	NIC_CMD_SEND_RECV_NOP = 2,
	NIC_CMD_COLL_OPS_LONG = 3,
	NIC_CMD_RESERVED1 = 4, // TODO: remove this after integration with HCL
	NIC_CMD_COLL_OPS_RECV_INORDER_V2 = 5,
	NIC_CMD_RESERVED2 = 6,
	NIC_CMD_RESERVED3 = 7,
	NIC_CMD_SEND_RECV = 8,
	NIC_CMD_SEND_RECV1 = 9, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV2 = 10, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV3 = 11, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV4 = 12, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV5 = 13, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV6 = 14, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_SEND_RECV7 = 15, /* This is same as NIC_CMD_SEND_RECV */
	NIC_CMD_COLL_OPS = 16,
	NIC_CMD_COLL_OPS1 = 17,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS2 = 18, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS3 = 19, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS4 = 20, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS5 = 21, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS6 = 22, /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS7 = 23,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS8 = 24,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS9 = 25,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS10 = 26,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS11 = 27,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS12 = 28,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS13 = 29,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS14 = 30,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_CMD_COLL_OPS15 = 31,  /* This is same as NIC_CMD_COLL_OPS */
	NIC_ENG_ARC_CMD_COUNT = 32,
	NIC_ENG_ARC_CMD_SIZE = 0x1F
};

/**
 * \enum    nic_scaleout_eng_arc_cmd_t
 * \brief   Various NIC scaleout commands
 * \details Command type enum specifies the commands that are processed by
 *	    NIC engines ARCs.
 */
enum nic_scaleout_eng_arc_cmd_t {
	NIC_SCALEOUT_CMD_COLL_OPS = 0,
	NIC_SCALEOUT_CMD_UPDATE_GLBL_CTXT = 1,
	NIC_SCALEOUT_ENG_ARC_CMD_COUNT = 2,
	NIC_SCALEOUT_ENG_ARC_CMD_SIZE = 0xF
};

/**
 * Total count of Work distribution context supported
 */
#define WD_CTXT_COUNT	8

#define EXPERT_MAPPING_CTXT_COUNT	2
#define EXPERT_MAPPING_ENTRY_COUNT	32
#define INVALID_EXPERT_MAPPING_ENTRY	0XFFFF

#define MAX_DIMENSIONS	5

#define TENSOR_DIM0	0
#define TENSOR_DIM1	1
#define TENSOR_DIM2	2
#define TENSOR_DIM3	3
#define TENSOR_DIM4	4

/**
 * DCCM buffer size for doing the DMA transfers
 * All the ECB commands should be within these boundaries
 */
enum {
	STATIC_COMPUTE_ECB_LIST_BUFF_SIZE = 256,
	DYNAMIC_COMPUTE_ECB_LIST_BUFF_SIZE = 256,
};
/**
 * \struct  tpc_virt_sob_id_t
 * \brief   virtual sob identifier for TPC
 * \details Virtual SOB ID for TPC that is shared between GC and Firmware.
 * 	    Each SO Set of the TPC is divided into two sub so sets.
 * 	    Threshold value to arm the monitor is always in the multiple of 32
 * 	    After completing work, each TPC increments the same SOB. Since
 * 	    there are large number of TPC, we can not use different SOBs for
 *	    each TPCs.
 */
struct tpc_virt_sob_id_t {
	union {
		struct {
			uint16_t sob_offset:5;
			/**<
			 * This field specifies offset to be added into sob
			 * start id of the sub soset
			 */
			uint16_t threshold:10;
			/*
			 * Monitor threshold to be programmed. This field is
			 * in the multiple of 32, so threshold = 1 means 32
			 * and 2 means 64 and so on.
			 */
			uint16_t :1;
			/**<
			 * UNSUPPORTED
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  mme_virt_sob_id_t
 * \brief   virtual sob identifier for MMEs
 * \details Virtual SOB ID for MMEs that is shared between GC and Firmware.
 * 	    Each SO Set of the MMEs is divided into two sub so sets.
 * 	    After completing work, each MME increments the its own SOB. Since
 * 	    there are only two master MMEs, we need two SOBs, there for each
 * 	    sub so set is grouped in pairs of two SOBs.
 * 	    SUB SO SET0 : [Pair0(SOB0, SOB1), Pair1(SOB2, SOB3),
 * 	                   Pair2(SOB4, SOB5), Pair3(SOB6, SOB7)],
 * 	    SUB SO SET1 : [Pair0(SOB8, SOB9), Pair1(SOB10, SOB11),
 * 		           Pair2(SOB12, SOB13) Pair3(SOB14, SOB15)]
 * 	    Due to this MME0 udates only SOBs with even offset and MME2 updates
 * 	    only odd offset SOBs
 * 	    one monitor monitors both the SOBs within a pair.
 */
struct mme_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:13;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t sob_pair_id:2;
			/**<
			 * SOBs within the sub so set is grouped into pair of
			 * two, as there are two MMEs. Each MME updates its own
			 * SOBs.
			 */
			uint16_t :1;
			/**<
			 * UNSUPPORTED
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  edma_virt_sob_id_t
 * \brief   virtual sob identifier for EDMAs
 * \details Virtual SOB ID for EDMAs that is shared between GC and Firmware.
 * 	    Each SO Set of the EDMAs is divided into two sub so sets.
 * 	    After completing work, each EDMAs increments the its own SOB. Since
 * 	    there are 5 EDMAs used in compute operation, we need 5 SOBs,
 * 	    there for each sub so set is grouped in tuple of 5 SOBs.
 *	    SUB SO SET0 : SOB0(EDMA0), SOB1(EDMA1), SOB2(EDMA2), SOB3(EDMA3),
 *			  SOB4(EDMA4), SOB5(Not Used), SOB6(Not Used), SOB7(Not Used)
 *	    SUB SO SET1 : SOB8(EDMA0), SOB9(EDMA1), SOB10(EDMA2), SOB11(EDMA3),
 *			  SOB12(EDMA4), SOB13(Not Used), SOB14(Not Used), SOB15(Not Used)
 * 	    Each EDMA udates only their own SOBs as described above.
 * 	    one monitor monitors all the 5 SOBs within a subset.
 */
struct edma_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t sub_soset_id:1;
			/**<
			 * one SO set of the EDMA is divided into two, known
			 * as sub so sets. This field specifies which sub so
			 * set to be used
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  rot_virt_sob_id_t
 * \brief   virtual sob identifier for Rotators
 * \details Virtual SOB ID for Rotators that is shared between GC and
 *	    Firmware. Each SO set of the ROT is divided into eight sub so sets.
 *		After completing work, each ROT increments its own SOB.
 *		SUB SO SET0: SOB0(ROT0), SOB1(ROT1)
 */
struct rot_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:13;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t sub_soset_id:3;
			/**<
			 * one SO set of the ROT is divided into eight, known
			 * as sub so sets. This field specifies which sub so
			 * set to be used
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  dbg_virt_sob_id_t
 * \brief   virtual sob identifier for debug purpose
 * \details Virtual SOB ID for debug purpose that is shared between GC and
 *          Firmware.
 */
struct dbg_virt_sob_id_t {
	union {
		struct {
			uint16_t threshold:15;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint16_t reserved:1;
			/*
			 * reserved
			 */
		};
		uint16_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  dbg_virt_long_sob_id_t
 * \brief   long virtual sob identifier for debug purpose
 * \details Long Virtual SOB ID for debug purpose that is shared between GC and
 *          Firmware.
 */
struct dbg_virt_long_sob_id_t {
	union {
		struct {
			uint32_t threshold:30;
			/*
			 * Monitor threshold to be programmed.
			 */
			uint32_t reserved:2;
			/*
			 * reserved
			 */
		};
		uint32_t raw;
		/*
		 * Raw value
		 */
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  virt_sob_id_t
 * \brief   virtual sob identifier for each engines
 * \details data structure to store virtual SOB IDs for each engine types
 */
struct virt_sob_id_t {
	uint16_t raw;
} __attribute__ ((aligned(2), __packed__));

/**
 * \struct  virt_sob_ids_t
 * \brief   virtual sob identifier for each engines
 * \details data structure to store virtual SOB IDs for each engine types
 */
struct virt_sob_ids_t {
	struct tpc_virt_sob_id_t tpc;
	/*
	 * Virtual SOB ID for TPC engines
	 */
	struct mme_virt_sob_id_t mme;
	/*
	 * Virtual SOB ID for MME engines
	 */
	struct edma_virt_sob_id_t edma;
	/*
	 * Virtual SOB ID for EDMA engines
	 */
	struct rot_virt_sob_id_t rot;
	/*
	 * Virtual SOB ID for ROTATOR engines
	 */
	struct dbg_virt_long_sob_id_t dbg;
	/*
	 * Virtual SOB ID for debug purpose
	 */
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  full_hbm_addr_ctxt_t
 * \brief   full hbm addr ctxt
 * \details full hbm addr used for patching
 */
struct full_hbm_addr_ctxt_t {
	union {
		uint64_t hbm_addr;
		struct {
			uint64_t addr_low:32;
			uint64_t addr_high:32;
		} __attribute__ ((aligned(4), __packed__));
	};
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  rot_wd_ctxt_t
 * \brief   Rotator specific work distribution context
 * \details Rotator work distribution context for GC
 */
struct rot_wd_ctxt_t {
	union {
		uint32_t word0;
		struct {
			uint32_t switch_bit:1;
			/**
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t cpl_msg_en:1;
			/**
			 * Enable Completion msg. Increment SOB only when this is set
			 */
			uint32_t reserved:6;
			/**
			 * reserved
			 */
			uint32_t sig_inc_value:16;
			/**
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:8;
			/**
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};

	struct virt_sob_ids_t virt_sob_ids;
	/**
	 * Virtual SOB array
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  rot_wd_patch_ctxts_t
 * \brief   ROT engine and sync scheme context
 * \details ROT engine context and sync scheme context that is patched dynamically
 */
struct rot_wd_patch_ctxts_t {
	struct full_hbm_addr_ctxt_t weight_base_address;
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  rot_wd_ctxts_t
 * \brief   Rotator engine and sync scheme context
 * \details Rotator engine context and sync scheme context used for GC
 */
struct rot_wd_ctxts_t {
	struct rot_wd_ctxt_t rot_ctxt[WD_CTXT_COUNT];
	struct rot_wd_patch_ctxts_t patch_ctxt[WD_CTXT_COUNT];
	uint16_t expert_mapping_ctxt[EXPERT_MAPPING_CTXT_COUNT * EXPERT_MAPPING_ENTRY_COUNT];
	/**<
	 * array of contexts for Rotator
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

enum mme_operand_type_t {
	MME_ADDR_A = 0,
	MME_ADDR_B = 1,
	MME_ADDR_COUT0 = 2,
	MME_OPERAND_COUNT = 3
};

/**
 * \struct  mme_wd_ctxt_t
 * \brief   MME specific work distribution context
 * \details MME work distribution context for GC
 */
struct mme_wd_ctxt_t {
	uint32_t mme_commit_reg;
	/**<
	 * mme commit register parameters
	 */
	union {
		uint32_t word0;
		struct {
			uint32_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t mme_operand:2;
			/**<
			 * mme operand to patch from mme_operand_type_t
			 */
			uint32_t reserved:5;
			/**<
			 * reserved
			 */
			uint32_t sig_inc_value:16;
			/**<
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:8;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
	struct full_hbm_addr_ctxt_t weight_offset[GAUDI2_MAX_MME_COUNT];
	/**<
	 * hbm addr offset of tensor for patching
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  mme_wd_patch_ctxts_t
 * \brief   mme wd patch ctxt
 * \details full hbm addr used for patching and fp8_bias value
 */
struct mme_wd_patch_ctxts_t {
	struct full_hbm_addr_ctxt_t weight_base_address;
	uint32_t fp8_bias_value;
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  mme_wd_ctxts_t
 * \brief   MME engine and sync scheme context
 * \details MME engine context and sync scheme context used for GC
 */
struct mme_wd_ctxts_t {
	struct mme_wd_ctxt_t mme_ctxt[WD_CTXT_COUNT];
	struct mme_wd_patch_ctxts_t patch_ctxt[WD_CTXT_COUNT];
	uint16_t expert_mapping_ctxt[EXPERT_MAPPING_CTXT_COUNT * EXPERT_MAPPING_ENTRY_COUNT];
	/**<
	 * array of contexts for MME
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \enum    edma_op_type_t
 * \brief   Various EDMA operations
 * \details EDMA enums supported by Firmware
 *          Note: In EDMA_OP_TRANSPOSE and EDMA_OP_NO_WD, FW only does
 *                sync scheme
 */
enum edma_op_type_t {
	EDMA_OP_MEMSET_TENSOR = 0,
	EDMA_OP_MEMSET_LINEAR = 1,
	EDMA_OP_MEMCPY_TENSOR = 2,
	EDMA_OP_MEMCPY_LINEAR = 3,
	EDMA_OP_TRANSPOSE = 4, /* TODO: Remove this later */
	EDMA_OP_NO_WD = 5,
	EDMA_OP_COUNT = 6
};

enum edma_operand_type_t {
	EDMA_SRC = 0,
	EDMA_DST = 1,
	EDMA_OPERAND_COUNT = 2
};

/**<
 * Total number of EDMA engines involved in compute
 */
#define ENG_EDMA_COMPUTE_COUNT	5

/**
 * \struct  edma_tensor_t
 * \brief   EDMA tensor data structure
 * \details Data structure to store EDMA tensor related parameters
 */
struct edma_tensor_t {
	uint32_t grid_size[MAX_DIMENSIONS];
	/**<
	 * Size of the complete tensor
	 * DIM0 is in Bytes, rest all in elements
	 */
	uint32_t box_size[MAX_DIMENSIONS];
	/**<
	 * Chunk size of the tensor
	 * DIM0 is in Bytes, rest all in elements
	 * Note: Using box size field, firmware programs the tsize registers
	 * of source and destination tensors
	 */
	uint64_t addr_offset;
	/**<
	 * address offset of the main tensor,
	 * for the individual boxes the offsets would
	 * calculated by FW using this field
	 * Note: Firmware programs SRC and DST offset registers by using this
	 * field along with other parameters specified in this structure
	 */
	uint32_t box_offset[ENG_EDMA_COMPUTE_COUNT];
	/**<
	 * Each Engine processes only one box from the given grid.
	 * This field provides the offset that should be used by the
	 * engine to know which box it should process
	 * Firmware calculates the engine specific box offset as
	 * Engine Box offset = box_offset[Engine Index]
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_wd_ctxt_t
 * \brief   EDMA specific work distribution context
 * \details EDMA work distribution context for GC
 */
struct edma_wd_ctxt_t {
	uint32_t dma_commit_reg;
	union {
		uint32_t word0;
		struct {
			/**<
			 * dma commit register value to be written
			 */
			uint32_t dma_op:3;
			/**<
			 * DMA operation to be performed from edma_op_type_t
			 */
			uint32_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint32_t shuffle_index:3;
			/**<
			 * Index of the 1st engine to start with, 3 bits
			 * Linear number starting from 0 to (num_engines - 1)
			 */
			uint32_t use_alternate_addr:1;
			/**<
			 * WA for this : SW-138366
			 * Whenever this flag is set firmware is expected to configure
			 * alternate address of RD_HBW_MAX_OUTSTAND as completion address
			 * value of 0 is set by the GC in the WR_COMP_WDATA
			 */
			uint32_t dma_operand:1;
			/**<
			 * Edma operand to patch from edma_operand_type_t
			 */
			uint32_t sig_inc_value:16;
			/**<
			 * Increment value to be added to previous threshold
			 */
			uint32_t virtual_sob_bitmap:7;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct edma_tensor_t dst_tensor;
	/**<
	 * Destination tensor configuration
	 */
	struct edma_tensor_t src_tensor;
	/**<
	 * Source tensor configuration
	 */
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
	struct full_hbm_addr_ctxt_t weight_offset[GAUDI2_MAX_EDMA_COUNT];
	/**<
	 * hbm addr offset of tensor for patching
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_wd_patch_ctxts_t
 * \brief   EDMA engine and sync scheme context
 * \details EDMA engine context and sync scheme context that is patched dynamically
 */
struct edma_wd_patch_ctxts_t {
	struct full_hbm_addr_ctxt_t weight_base_address;
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_wd_ctxts_t
 * \brief   EDMA engine and sync scheme context
 * \details EDMA engine context and sync scheme context used for GC
 */
struct edma_wd_ctxts_t {
	struct edma_wd_ctxt_t edma_ctxt[WD_CTXT_COUNT];
	struct edma_wd_patch_ctxts_t patch_ctxt[WD_CTXT_COUNT];
	uint16_t expert_mapping_ctxt[EXPERT_MAPPING_CTXT_COUNT * EXPERT_MAPPING_ENTRY_COUNT];
	/**<
	 * array of contexts for EDMA
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * EDMA NIC maximum dwords in glbl ctxt
 */
enum {
	EDMA_NIC_SIBO_BASE_COUNT = 2,
	EDMA_NIC_MAX_DWORDS_IN_CTXT = 6,
	EDMA_NIC_COMP_CFG_MAX = 8,
	EDMA_NIC_MAX_DWORDS_IN_CTXT_V2 = 9,
	EDMA_NIC_MAX_DWORDS_IN_CTXT_V3 = 17
};

/*
 * DWORD position of different members of EDMA glbl ctxt
 */
enum {
	EDMA_NIC_SIB_BASE_ADDR_LO_0 = 0,
	EDMA_NIC_SIB_BASE_ADDR_HI_0 = 1,
	EDMA_NIC_SIB_BASE_ADDR_LO_1 = 2,
	EDMA_NIC_SIB_BASE_ADDR_HI_1 = 3,
	EDMA_NIC_SIB_RANK_STRIDE_0 = 4,
	EDMA_NIC_SIB_RANK_STRIDE_1 = 5,
	EDMA_NIC_SIRB_BASE_ADDR_LO = 6,
	EDMA_NIC_SIRB_BASE_ADDR_HI = 7,
	EDMA_NIC_SIRB_SIZE = 8,
	EDMA_NIC_COMP_CFG_0 = 9,
	EDMA_NIC_COMP_CFG_1 = 10,
	EDMA_NIC_COMP_CFG_2 = 11,
	EDMA_NIC_COMP_CFG_3 = 12,
	EDMA_NIC_COMP_CFG_4 = 13,
	EDMA_NIC_COMP_CFG_5 = 14,
	EDMA_NIC_COMP_CFG_6 = 15,
	EDMA_NIC_COMP_CFG_7 = 16,
	EDMA_NIC_UPDATE_BITMAP_SIZE  = EDMA_NIC_MAX_DWORDS_IN_CTXT_V3
};

/**
 * \struct  edma_nic_glbl_ctxt_v3_t
 * \brief   EDMA NIC global context
 * \details This data structure contains information related to buffers
 *	    which are required by various EDMA cmds
 */
struct edma_nic_glbl_ctxt_v3_t {
	union {
		struct {
			uint64_t sib_base_addr[EDMA_NIC_SIBO_BASE_COUNT];
			/**<
			 * Address array of Static Intermediate Buffer for storing incoming
			 * data in order
			 * Actual Address: sib_order_base_addr + (sibo_rank_stride * 8) * index
			 */
			uint32_t sibo_rank_stride[EDMA_NIC_SIBO_BASE_COUNT];
			/**<
			 * Array of sib_order Rank stride, in other words maximum size of
			 * sib_order sub buffers for each rank
			 */
			uint64_t sirb_base_addr;
			/**<
			 * Base address of Static Intermediate reduction Buffer (sirb) to store
			 * reduction data in order
			 */
			uint32_t sirb_size;
			/**<
			 * Total size of intermediate reduction buffer (sirb size)
			 */
			uint32_t comp_cfg[EDMA_NIC_COMP_CFG_MAX];
			/**<
			 * Completion group cfg base that would be used to derive the completion
			 * SOB base address
			 */
		};
		uint32_t raw_dwords[EDMA_NIC_MAX_DWORDS_IN_CTXT_V3];
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  edma_nic_wd_ctxts_t
 * \brief   EDMA NIC engine WD context
 * \details EDMA engine context used for WD
 */
struct edma_nic_wd_ctxts_t {
	struct edma_nic_glbl_ctxt_v3_t edma_nic_ctxt_v3;
	/**<
	 * contexts for NIC EDMA.
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  index_space_tensor_t
 * \brief   Data structure which defines index space tensor
 * \details Index space tensor parameters stored within GC Context
 */
struct index_space_tensor_t {
	uint32_t base_cord[MAX_DIMENSIONS];
	/**<
	 * Base coordinate of the grid
	 */
	uint32_t grid_size[MAX_DIMENSIONS];
	/**<
	 * Actual or Total Size of the index space tensor
	 */
	uint32_t box_size[MAX_DIMENSIONS];
	/**<
	 * Index space tensor is divided into small boxes
	 * Each box is processed by a particular TPC.
	 * Box size is nothing bug amount of work
	 * that is processed by a TPC.
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  tpc_wd_ctxt_t
 * \brief   TPC specific work distribution context
 * \details TPC specific work distribution context for GC
 */
struct tpc_wd_ctxt_t {
	struct index_space_tensor_t ist;
	/**<
	 * Index space tensor parameters
	 */
	union {
		uint32_t word1;
		struct {
			uint8_t shuffle_index;
			/**<
			 * Index of the 1st engine to start with, 5 bits
			 * Linear number starting from 0 to (num_engines - 1)
			 */
			uint8_t switch_bit:1;
			/**<
			 * value of the switch bit to be configured when pushing the
			 * descriptor into ARC CQ
			 */
			uint8_t reserved:3;
			/**<
			 * Reserved
			 */
			uint8_t enable_two_boxes:1;
			/**<
			 * Enable variable DIMs as an optimization to increase TPC
			 * usage efficiency
			 */
			uint8_t two_box_dim_index:3;
			/**<
			 * DIM_INDEX to be decremented by 1. Possible values: [0, 1, 2, 3, 4]
			 */
			uint8_t virtual_sob_bitmap;
			/**<
			 * Virtual SOB bitmap indicating index which are valid
			 * in the virtual_sob array
			 */
			uint8_t two_box_tpc_index;
			/**<
			 * Index from which the variation in DIMs needs to be applied.
			 * This is counted from shuffle_index
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	union {
		uint32_t word2;
		struct {
			uint16_t tensor_id:4;
			/**<
			 * tpc operand to patch (0-15)
			 */
			uint16_t tensor_id2:4;
			/**<
			 * tpc operand to patch (0-15)
			 */
			uint16_t reserved1:8;
			/**<
			 * reserved
			 */
			uint16_t sig_inc_value;
			/**<
			 * Increment value to be added to previous threshold
			 */
		} __attribute__ ((aligned(4), __packed__));
	};
	struct virt_sob_ids_t virt_sob_ids;
	/**<
	 * Virtual SOB array
	 */
	struct full_hbm_addr_ctxt_t weight_offset;
	/**<
	 * hbm addr offset of tensor for patching
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  tpc_wd_patch_ctxts_t
 * \brief   TPC engine and sync scheme context
 * \details TPC engine context and sync scheme context that is patched dynamically
 */
struct tpc_wd_patch_ctxts_t {
	struct full_hbm_addr_ctxt_t weight_base_address;
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  tpc_wd_ctxts_t
 * \brief   TPC engine and sync scheme context
 * \details TPC engine context and sync scheme context used for GC
 */
struct tpc_wd_ctxts_t {
	struct tpc_wd_ctxt_t tpc_ctxt[WD_CTXT_COUNT];
	struct tpc_wd_patch_ctxts_t patch_ctxt[WD_CTXT_COUNT];
	uint16_t expert_mapping_ctxt[EXPERT_MAPPING_CTXT_COUNT * EXPERT_MAPPING_ENTRY_COUNT];
	/**<
	 * Array of contexts for TPC
	 */

	/*
	 * TODO: Add global parameters here
	 * Global means used in all the contexts
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_generic_t
 * \brief   generic command structure
 * \details Generic command structure
 */
struct eng_arc_cmd_generic_t {
	uint32_t cmd_type:4;
	/**<
	 * set to eng_arc_cmd_t
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start
	 */
	uint32_t reserved:24;
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  eng_arc_cmd_nop_t
 * \brief   NOP command structure
 * \details NOP command to align other commands on the buffer boundary
 */
struct eng_arc_cmd_nop_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_NOP
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ
	 * Engine FW pushes a NOP QMAN command into Static CQ or ARC CQ
	 * depending on the ECB list where this command is encountered
	 * and causes switch between Static CQ and ARC CQ
	 */
	uint32_t padding:23;
	/**<
	 * Number of DWORDS(4 Bytes) padded after this command.
	 * When padding = 0 means the size of DWORD command is 1 DWORD
	 */
} __attribute__ ((aligned(4), __packed__));

#define CPU_INDEX_ALL			0xFE
#define CPU_INDEX_INVALID		0xFF

/**
 * \struct  eng_arc_cmd_static_desc_v2_t
 * \brief   Static CP DMA transfer
 * \details Push a descriptor into static CQ. The content of the descriptor
 *	    is not known to Firmware. The descriptor buffer can contain
 *	    any valid QMAN commands
 */
struct eng_arc_cmd_static_desc_v2_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_STATIC_DESC_V2
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t reserved:3;
	/**<
	 * Reserved, unused
	 */
	uint32_t cpu_index:8;
	/**<
	 * Virtual CPU Index from 0 to N-1, where N is number of engines
	 *
	 * cpu_index = CPU_INDEX_ALL command is processed by all engine ARCs
	 * cpu_index = CPU_INDEX_INVALID command is ignored by all engine ARCs
	 */
	uint32_t size:13;
	/**<
	 * transfer size in bytes
	 * TODO: can be coverted to DWORD if 21bits are not enough
	 */
	uint32_t addr_index:3;
	/**<
	 * Recipe base address register index to be used to generate target
	 * address of 64 bits
	 */
	uint32_t addr_offset;
	/**<
	 * 32bit address offset
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \enum    signaling_completion_type_t
 * \brief   completion signal sent to sob by firmware
 * \details completion signal sent to sob by firmware
 */
enum signaling_completion_type_t {
	SIGNAL_TO_SYNC_SCHEME_SOB = 0x0,
	SINGAL_TO_AUX_REG = 0x1,
	SINGAL_COUNT = 0x2
};

/**
 * \struct  eng_arc_cmd_wd_fence_and_exec_t
 * \brief   Work distribution, fence and execute
 * \details TODO;
 */
struct eng_arc_cmd_wd_fence_and_exec_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_WD_FENCE_AND_EXE
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start.
	 * Expected value is 1.
	 */
	uint32_t dma2_completion:3;
	/**<
	 * Number of DMAs should complete before the execution can start.
	 * This wait is for dma waiting for dma. Can have 0 or more value.
	 */
	uint32_t wd_ctxt_id:3;
	/**<
	 * a context number from 0 to max number of contexts that fw supports
	 */
	uint32_t wd_ctxt2_id:3;
	/**<
	 * a context number from 0 to max number of weight_base_address contexts
	 */
	uint32_t patch_address:1;
	/**<
	 * Patch address before execution (Tensor ID 1)
	 */
	uint32_t signal_arc:1;
	/**<
	 * which sob to signal from signaling_completion_type_t
	 */
	uint32_t expert_mapping_idx: 6;
	/**<
	 * expert mapping index
	 */
	uint32_t conditional_activation:1;
	/**<
	 * conditional_activation
	 */
	uint32_t patch_address2:1;
	/**<
	 * Patch address before execution (Tensor ID 2)
	 */
	uint32_t :5;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_sched_dma_t
 * \brief   Schedule DMA to update GC context
 * \details Initiate a DMA transfer to update GC context. Any command which
 *	    depends on this transfer to be completed can use the dma_completion
 *	    field of that command to wait until GC context is updated.
 */
struct eng_arc_cmd_sched_dma_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_SCHED_DMA
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t addr_index:3;
	/**<
	 * Recipe base address register index to be used to generate target
	 * address of 64 bits
	 */
	uint32_t size:10;
	/**<
	 * size of the buffer in bytes
	 */
	uint32_t gc_ctxt_offset:14;
	/*
	 * destination address where the DMA needs to be done
	 * offset of the location within GC managed struct in the DCCM
	 */
	uint32_t addr_offset;
	/**<
	 * 32bit address offset into recipe base address
	 */
} __attribute__ ((aligned(4), __packed__));


/**
 * DMA type
 */
enum dma_type_t {
	DMA_EXPERT_MAPPING_TABLE = 0x0,
	DMA_HBM_TENSOR_ADDR = 0x1,
	DMA_COUNT = 0x2
};

/**
 * \struct  eng_arc_cmd_sched_dma_v2_t
 * \brief   Schedule DMA  version 2 to update GC context
 * \details Initiate a DMA transfer to update expert mapping context.
 */
struct eng_arc_cmd_sched_dma_v2_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_SCHED_DMA_V2
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t dma_completion:3;
	/**<
	 * Number of DMAs should complete before starting this DMA
	 */
	uint32_t addr_index:3;
	/**<
	 * Recipe base address register index to be used to generate
	 * target address of 64 bits
	 */
	uint32_t size:8;
	/**<
	 * size of the buffer in bytes
	 */
	uint32_t dma_type:1;
	/*
	 * What needs to be dma from dma_type_t
	 * 0 - DMA_EXPERT_MAPPING_TABLE
	 * 1 - DMA_HBM_TENSOR_ADDR
	 */
	uint32_t wait_for_eng:1;
	/*
	 * Wait for a signal from Engine
	 */
	uint32_t expert_mapping_idx: 6;
	/**<
	 * expert mapping index
	 */
	uint32_t :2;
	/*
	 * Reserved
	 */
	uint32_t wd_ctxt_id:3;
	/*
	 * GC Context ID that needs to be updated
	 * This is used to calculate Destination Address
	 */
	uint32_t addr_offset;
	/**<
	 * 32bit address offset into recipe base address
	 */
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  eng_arc_cmd_sfg_t
 * \brief   Signal From Graph
 * \details Signals to a particular SOB outside of the graph sync scheme.
 *	    This can be used to synchronize compute stream with network stream
 *	    without waiting for the completion of entire graph.
 */
struct eng_arc_cmd_sfg_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_SFG
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ bit is used by firmware to set the switch_bit
	 * in the last QMAN command to switch the CQ
	 */
	uint32_t reserved:11;
	/**<
	 * Reserved
	 */
	uint32_t sob_inc_value:15;
	/**<
	 * The value that should be used to increment the SOB
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_reset_soset_t
 * \brief   Reset SO Set
 * \details Reset active sync object set when one of the SOs overflows.
 */
struct eng_arc_cmd_reset_soset_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_RESET_SOSET
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t switch_cq:1;
	/**<
	 * Switch CQ bit is used by firmware to set the switch_bit
	 * in the last QMAN command to switch the CQ
	 */
	uint32_t num_cmpt_engines:10;
	/**<
	 * Total no. of active physical compute engines
	 */
	uint32_t target:16;
	/**<
	 * Based on the engine type this value is either of the following
	 * struct edma_virt_sob_id_t
	 * struct mme_virt_sob_id_t
	 * struct tpc_virt_sob_id_t
	 * struct rot_virt_sob_id_t
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  eng_arc_cmd_list_size_t
 * \brief   Schedule DMA to update GC context
 * \details This is the first command in the chunk to indicate size of the list
 *	    Its present in static and dynamic list both.
 */
struct eng_arc_cmd_list_size_t {
	uint32_t cmd_type:4;
	/**<
	 * set to ECB_CMD_LIST_SIZE
	 */
	uint32_t yield:1;
	/**<
	 * Yield ARC control to the other list (s/d) after execution
	 */
	uint32_t topology_start:1;
	/**<
	 * start of new topology, fw can reset
	 * prev_sob_id etc. when this flag is set to 1
	 */
	uint32_t reserved:2;
	/**<
	 * reserved
	 */
	uint32_t list_size:24;
	/**<
	 * Total size of list in bytes; for FW management of double buffer
	 * The size includes the size of this command as well.
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * Total number of NIC collective contexts stored in Fimrware
 * 0 to 7 are RECV context
 * 8 to 15 are SEND context
 */
enum {
	SCALEUP_NIC_COLL_CTXT_COUNT = 16,

	SCALEOUT_NIC_COLL_CTXT_COUNT = 16
};

/**
 * NIC collective ops strategy to be used
 */
enum nic_coll_strategy_t {
	NIC_STRATEGY_ZERO = 0,
	/**<
	 * offset += 0.
	 */
	NIC_STRATEGY_REMOTE_RANK = 1,
	/**<
	 * offset += stride * remote_index
	 */
	NIC_STRATEGY_LOCAL_RANK = 2
	/**<
	 * offset += stride * local_index
	 */
};

/**
 * \struct  nic_glbl_ctxt_t
 * \brief   NIC global context
 * \details Each NIC engine ARC Firmware maintains global context
 */
struct nic_glbl_ctxt_t {
	uint32_t total_nic_count:5;
	/**<
	 * Total number of ports. In case of scaleout this can have a value between 1
	 * and MAX. In case of scaleup it will always have MAX number of ports.
	 */
	uint32_t is_valid:1;
	/**<
	 * Boolean. whether this global context is valid.
	 */
	uint32_t remote_dev_idx:3;
	/**<
	 * The remote deviceâ€™s accel index (/dev/accel/accel%d) that is
	 * connected to this device. Don't care for scaleout NICs
	 */
	uint32_t sub_nic_idx:3;
	/**<
	 * The sub-NIC index for this NIC. Range: [0, 2].
	 */
	uint32_t ports_per_rank:3;
	/**<
	 * active ports that can be used for nic operations. HCL can set this field
	 * during runtime based on the active number of ports that it intends to use
	 */
	uint32_t reserved1:17;
	/**<
	 * Reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * NIC maximum dwords in glbl ctxt
 */
enum {
	NIC_MAX_SOB_POOLS = 32,
	NIC_MAX_DWORDS_IN_CTXT = 38,
};

/**
 * \struct nic_sob_pool_info_t
 * \brief  NIC completion group info
 * \details This data structure contains information of the
 *         completion group base and sob count
 */
struct nic_sob_pool_info_t {
	uint32_t sob_base_index:13;
	/**<
	 * Identifies the SOB from the 8192 SOBs in a dcore
	 */
	uint32_t dcore_id:2;
	/**<
	 * Identifies dcore from the 4 dcores in a NIC
	 */
	uint32_t sob_count:8;
	/**<
	 * number of SOBs present in this sob_pool
	 * this will be used to handle wrap around
	 * scenarios in firmware
	 */
	uint32_t :9;
	/**<
	 * Reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  nic_glbl_ctxt_v2_t
 * \brief   NIC global context
 * \details This data structure contains information related to buffers
 *	    which are required in reproduceable reduction
 */
struct nic_glbl_ctxt_v2_t {
	union {
		struct {
			uint64_t sib_order_base_addr;
			/**<
			 * Address of Static Intermediate Buffer for storing incoming
			 * data in order
			 * Actual Address: sib_order_base_addr + (sibo_rank_stride * 8) * index
			 */
			uint64_t reserved; /* added hcl jobs are failing due to hcl using master code in CI */

			uint32_t sibo_rank_stride;
			/**<
			 * sib_order Rank stride, in other words maximum size of sib_order sub buffers
			 * fpr each rank
			 */
			uint32_t reserved2;/* added hcl jobs are failing due to hcl using master code in CI */


			struct nic_sob_pool_info_t sob_pool[NIC_MAX_SOB_POOLS];
			/**<
			 * NIC sob pool info
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t raw_dwords[NIC_MAX_DWORDS_IN_CTXT];
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  nic_ctxt_dword
 * \brief   Dword in the NIC context
 * \details
 */
struct nic_coll_ctxt_dword_t {
	union {
		struct {
			uint32_t remote_rank_index0:3;
			/**<
			 * Rank index of the neighbour 0 for this
			 * collective context
			 */
			uint32_t remote_rank_enable0:1;
			/**<
			 * Whether neighbour 0 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index1:3;
			/**<
			 * Rank index of the neighbour 1 for this
			 * collective context
			 */
			uint32_t remote_rank_enable1:1;
			/**<
			 * Whether neighbour 1 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index2:3;
			/**<
			 * Rank index of the neighbour 2 for this
			 * collective context
			 */
			uint32_t remote_rank_enable2:1;
			/**<
			 * Whether neighbour 2 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index3:3;
			/**<
			 * Rank index of the neighbour 3 for this
			 * collective context
			 */
			uint32_t remote_rank_enable3:1;
			/**<
			 * Whether neighbour 3 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index4:3;
			/**<
			 * Rank index of the neighbour 4 for this
			 * collective context
			 */
			uint32_t remote_rank_enable4:1;
			/**<
			 * Whether neighbour 4 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index5:3;
			/**<
			 * Rank index of the neighbour 5 for this
			 * collective context
			 */
			uint32_t remote_rank_enable5:1;
			/**<
			 * Whether neighbour 5 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index6:3;
			/**<
			 * Rank index of the neighbour 6 for this
			 * collective context
			 */
			uint32_t remote_rank_enable6:1;
			/**<
			 * Whether neighbour 6 is part of this
			 * collective context
			 */
			uint32_t remote_rank_index7:3;
			/**<
			 * Rank index of the neighbour 7 for this
			 * collective context
			 */
			uint32_t remote_rank_enable7:1;
			/**<
			 * Whether neighbour 7 is part of this
			 * collective context
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t dword_value;
	};
} __attribute__ ((aligned(4), __packed__));

#define NIC_COMM_GROUP_DESC_ENTRY_COUNT	11

/**
 * \struct  nic_comm_desc_t
 * \brief   NIC communicator descriptor
 * \details NIC communicator descriptor table entry.
 */
struct nic_comm_desc_t {
	union {
		struct {
			uint32_t qpn:16;
			/**<
			 * QP index to use for completion
			 */
			uint32_t sob_pool_index:5;
			/**<
			 * Indexes into sob_pool_info array in nic_sob_pool_info_t
			 */
			uint32_t remote_rank_index:3;
			/**<
			 * The remote rank the NIC shall communicate with or the
			 * chunk index the NIC shall send
			 */
			uint32_t connection_enabled:1;
			/**<
			 * bit to tell whether connection is enabled
			 */
			uint32_t reduction_ind_local:1;
			/**<
			 * bit to tell whether reduction is enabled on this comm descriptor table
			 * entry
			 */
			uint32_t sob_inc_delta:6;
			/**<
			 * Increment sob_addr by sob_inc_delta, if use_sob_inc_delta is set
			 * else increment only once
			 * use sos_num to wrap around to correct overflow of sob address
			 */
		} __attribute__ ((aligned(4), __packed__));
		struct {
			uint32_t reserved0:21;
			uint32_t remote_idx_and_conn_enabled :4;
			uint32_t reserved1:7;
		};
		uint32_t comm_desc_raw;
	} __attribute__ ((aligned(4), __packed__));
} __attribute__ ((aligned(4), __packed__));


/**
 * \enum    nic_comp_type_t
 * \brief   NIC Completion types
 * \details Various completion types supported by NIC
 */
enum nic_comp_type_t {
	NIC_COMP_NO_SOB_NO_CQ = 0,
	NIC_COMP_ONLY_SOB = 1,
	NIC_COMP_ONLY_CQ = 2,
	NIC_COMP_SOB_AND_CQ = 3 /* Not supported in FW */
};

/**
 * \struct  nic_sob_t
 * \brief   NIC SOB encoding
 * \details SOB Bitfields for SOB addresses in the collective context
 */
struct nic_sob_t {
	union {
		struct {
			uint32_t sob_addr:27;
			/**<
			 * SOB Address to be incremented
			 */
			uint32_t so_data:2;
			/*
			* SO Data to be written on the SOB
			*/
			uint32_t is_recv:1;
			/*
			* Flag to indicate send/recv operation
			* 0 - Send Operation
			* 1 - Recv Operation
			*/
			uint32_t completion_type:2;
			/**<
			 * Refer to nic_comp_type_t
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t sob_raw;
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  nic_coll_ctxt_t
 * \brief   NIC Collective context
 * \details Each NIC engine ARC Firmware maintains SCALEUP_NIC_COLL_CTXT_COUNT
 *	    collective contexts which can be independently updated by
 *	    HCL.
 */
struct nic_coll_ctxt_t {
	union {
		struct {
			union {
				struct {
					uint16_t reduction_opcode:10;
					/**<
					 * This field is part of DWORD0.
					 * Reduction opcode. Does not include SE,
					 * in_line and ackreq
					 */
					uint16_t reserved1:6;
					/*
					 * Reserved
					 */
					uint16_t reserved2;
					/**<
					 * This field is part of DWORD0.
					 */
				} __attribute__ ((aligned(4), __packed__));
				uint32_t raw_dword0;
			};
			uint32_t sync_object_address_0;
			/**<
			 * This field is part of DWORD1.
			 * LBW address of the SOB that should be used by Firmware
			 * to program into NIC
			 * Engine ARC Firmware moves to the next SOB address
			 * based on the sob_index field of SCHED_ARC_CMD_NIC_COLL_OPS command
			 * sob address 0 or 1 is picked.
			 * Refer to struct nic_sob_t
			 */
			union {
				struct {
					uint32_t pre_inc_sob:1;
					/**<
					 * 0 - use SOB address and then increment
					 * 1 - increment SOB address and then use it
					 * TODO - Once HCL moves to always pre-increment SOB,
					 * this struct has to be cleanedup
					 */
					uint32_t :31;
					/**<
					 * reserved
					 */
				} __attribute__ ((aligned(4), __packed__));
				uint32_t sync_object_address_1;
				/**<
				 * This field is part of DWORD2.
				 * Refer to struct nic_sob_t
				 */
			};

			uint32_t buffer_addr_msb;
			/**<
			 * This field is part of DWORD3.
			 * Upper 32bits of the buffer.
			 */
			uint32_t stride;
			/**<
			 * This field is part of DWORD4.
			 * Stride to be used for a rank in the buffer
			 */
			struct nic_comm_desc_t comm_group_table[NIC_COMM_GROUP_DESC_ENTRY_COUNT];
			/**<
			 * NIC communication group descriptor table
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t dword[16];
	};
} __attribute__ ((aligned(4), __packed__));


/**
 * \struct  nic_coll_ctxt_scaleout_t
 * \brief   NIC Collective context for scaleout
 * \details Each NIC engine ARC Firmware maintains SCALEOUT_NIC_COLL_CTXT_COUNT
 *	    collective contexts which can be independently updated by
 *	    HCL.
 */
struct nic_coll_ctxt_scaleout_t {
	union {
		struct {
			union {
				struct {
					uint16_t reduction_opcode:10;
					/**<
					 * This field is part of DWORD0.
					 * Reduction opcode. Does not include
					 * SE, inline and ackreq
					 */
					uint16_t reserved1:6;
					/*
					 * reserved
					 */
					uint16_t reserved2;
					/**<
					 * This field is part of DWORD0.
					 */
				} __attribute__ ((aligned(4), __packed__));
				uint32_t raw_dword0;
			};
			uint32_t sync_object_address_0;
			/**<
			 * This field is part of DWORD1.
			 * LBW address of the SOB that should be used by Firmware
			 * to program into NIC
			 * Engine ARC Firmware moves to the next SOB address
			 * based on the sob_index field of SCHED_ARC_CMD_NIC_COLL_OPS command
			 * sob address 0 or 1 is picked.
			 */
			uint32_t sync_object_address_1;
			/**<
			 * This field is part of DWORD2.
			 */
			uint32_t buffer_addr_msb;
			/**<
			 * This field is part of DWORD3.
			 * Upper 32bits of the buffer.
			 */
			uint32_t stride;
			/**<
			 * This field is part of DWORD4.
			 * Stride to be used for a rank in the buffer
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t dword[5];
	};
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  nic_wd_ctxts_t
 * \brief   NIC engine and sync scheme context
 * \details NIC engine context and sync scheme context used for GC
 */
struct nic_wd_ctxts_t {
	struct nic_coll_ctxt_t coll_ctxt[SCALEUP_NIC_COLL_CTXT_COUNT];
	/**<
	 * Collective contexts for NIC
	 * used in NIC work distribution
	 */
	struct nic_glbl_ctxt_t glbl_ctxt;
	/**<
	 * Global context for NIC
	 * used in NIC work distribution
	 */
	struct nic_glbl_ctxt_v2_t glbl_ctxt_v2;
	/**<
	 * Global context for NIC
	 * used in reproduceable reduction
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  nic_wd_ctxts_scaleout_t
 * \brief   Scaleout NIC engine and sync scheme context
 * \details Scaleout NIC engine context and sync scheme context used for GC
 */
struct nic_wd_ctxts_scaleout_t {
	struct nic_coll_ctxt_scaleout_t coll_ctxt[SCALEOUT_NIC_COLL_CTXT_COUNT];
	/**<
	 * Collective contexts for NIC
	 * used in NIC work distribution
	 */
	struct nic_glbl_ctxt_t glbl_ctxt;
	/**<
	 * Global context for NIC
	 * used in NIC work distribution
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  arc_cmd_update_glbl_ctxt_t
 * \brief   Update the global ctxt for NICs
 * \details Update the global ctxt for Scaleup NICs
 */
struct arc_cmd_update_glbl_ctxt_t {
	union {
		struct {
			uint32_t reserved:13;
			/**<
			 * Reserved
			 */
			uint32_t update_glbl_ctxt_v2:1;
			/**<
			 * If this bit is set, update only nic_glbl_ctxt_v2_t
			 */
			uint32_t start_nic_idx:5;
			/**<
			 * NIC index from which nic_glbl_ctxt_t are provided.
			 * Updates will be made to glbl ctxt structure starting
			 * from start_nic_index
			 */
			uint32_t num_dwords:8;
			/**<
			 * Number of dwords present in this command
			 * payload
			 */
			uint32_t nic_opcode:5;
			/**<
			 * NIC opcode : NIC_CMD_UPDATE_GLBL_CTXT
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t raw_dword0;
	};
} __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_UPDATE_GLBL_CTXT_SIZE (sizeof(struct arc_cmd_update_glbl_ctxt_t))
#define ARC_CMD_UPDATE_GLBL_CTXT_SIZE_DWORD (ARC_CMD_UPDATE_GLBL_CTXT_SIZE / 4)

/**
 * \struct  arc_scaleout_cmd_update_glbl_ctxt_t
 * \brief   Update the global ctxt for NICs
 * \details Update the global ctxt for Scaleout NICs
 */
struct arc_scaleout_cmd_update_glbl_ctxt_t {
	uint32_t reserved:22;
	/**<
	 * Reserved
	 */
	uint32_t start_nic_idx:3;
	/**<
	 * Scaleout NIC index for which nic_glbl_ctxt_t are provided.
	 * Updates will be made to glbl ctxt structure starting
	 * from start_nic_index. Valid values = 0, 1 and 2
	 */
	uint32_t num_glbl_ctxt:3;
	/**<
	 * Number of uint32_t elements in the glbl_ctxt[] array
	 * present in this command. Max value for scaleout NIC is 3
	 */
	uint32_t nic_opcode:4;
	/**<
	 * NIC opcode : NIC_SCALEOUT_CMD_UPDATE_GLBL_CTXT
	 */
} __attribute__ ((aligned(4), __packed__));

#define ARC_SCALEOUT_CMD_UPDATE_GLBL_CTXT_SIZE (sizeof(struct arc_scaleout_cmd_update_glbl_ctxt_t))

/**
 * \struct  arc_cmd_update_coll_ctxt_t
 * \brief   Update the collective ctxt for NICs
 * \details Update the collective ctxt for NICs
 */
struct arc_cmd_update_coll_ctxt_t {
	uint32_t update_bitmap:5;
	/**<
	 * Bitmap of the DWORDs, which needs to be updated
	 * Bit 0 to 4 - Used for updating dwords 0 to 4
	 * of collective ctxt
	 */
	uint32_t update_qpn:1;
	/**<
	 * Bitmap to indicate QPN in the communicator descriptor table needs to be updated.
	 * QPN is a part of the dwords received as a part of command
	 */
	uint32_t update_rri_ce:1;
	/**<
	 * bitmap to indicate update of remote_rank and connection enabled fields
	 * in the communicator descriptor table.
	 */
	uint32_t update_reduction_ind_local:1;
	/**<
	 * bitmap to indicate update of local reduction indication bit
	 * in the communicator descriptor table.
	 */
	uint32_t update_sob_inc_delta:1;
	/**<
	 * Update SOB inc delta in the communicator descriptor table
	 */
	uint32_t update_sob_pool_index:1;
	/**<
	 * Update SOB pool index in global context
	 */
	uint32_t reserved0:1;
	/**<
	 * reserved
	 */
	uint32_t comm_desc_index:4;
	/**<
	 * index into nic_comm_desc_t
	 */
	uint32_t num_dwords:8;
	/**<
	 * Number of DWORDS to be updated by this command
	 */
	uint32_t ctxt_id:4;
	/**<
	 * Context ID, that needs to be updated by this command
	 */
	uint32_t nic_opcode:5;
	/**<
	 * opcode of the command = NIC_CMD_UPDATE_COLL_CTXT
	 */
} __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_UPDATE_COLL_CTXT_SIZE (sizeof(struct arc_cmd_update_coll_ctxt_t))
#define ARC_CMD_UPDATE_COLL_CTXT_SIZE_DWORD (ARC_CMD_UPDATE_COLL_CTXT_SIZE / 4)

#define NIC_CACHE_LINE_SIZE_IN_ELEMENTS 	64

/**
 * \struct  arc_cmd_coll_ops_short_t
 * \brief   Perform send/recv short operation on NICs
 * \details This command is used to perform send/recv short operations by user.
 *          Following is an example of how the values are calculated:
 *          Total transfer = cell_size = 1023 elements
 *          transfer size = 4092 if we consider FP32 as datatype
 *          cache line - 64 (does not depend on data type)
 *          cache_line_count having 64 elements each = 6 ciel(ciel(1023/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
 *          cache_line_remainder = 2 ((ceil(1023/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ? 0 : 3 - (ceil(cell_size/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
 *          element_remainder in number of elements = 1 (1 element of FP32)
 *
 *          while transferring data:
 *          NIC0 or NIC1
 *              nic_size = (6 * 64) * data_type;
 *          NIC2
 *              nic_size = (((6 - 2) * 64) - element_remainder) * data_type;
 *
 *          Note: There is a special case when cache_line_count <= 3. In that case entire data is
 *          sent by NIC0 and other NICs do not perform any transaction. They however still update
 *          the SOBs
 */
struct arc_cmd_coll_ops_short_t {
	uint32_t cache_line_count:13;
	/**<
	 * Amount of data in multiples of cache line size that each
	 * NIC needs to send.
	 * Value equal to ceil(ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
	 */
	uint32_t cache_line_remainder:3;
	/**<
	 * Remainder to be subtracted from cache_line_count value to calculate
	 * the size of the data to be sent by NIC.
	 * ((ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ? 0 : 3 - (ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
	 */
	uint32_t element_remainder:6;
	/**<
	 * remainder in terms of number of elements when the data is not integer
	 * multiple of cache line. Typically used by the last nic in the sub group.
	 * ((cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS) == 0) ? NIC_CACHE_LINE_SIZE_IN_ELEMENTS - (cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS)
	 *
	 * where cell_size is in terms of number of elements and it represents total
	 * elements that need to be transferred
	 */
	uint32_t force_remote_rank_offset:1;
	/**<
	 * This flag is used to force rank-offset to 0 during data transfer. This is not used for
	 * nic_size calculation and is applied after that. This means each sub nic will calculate
	 * offset only for within the rank but not between remote ranks.
	 */
	uint32_t use_sob_inc_delta:1;
	/**<
	 * If use_sob_inc_delta is set, increment SOB ID of collective context by delta,
	 * else increment only once
 	 */
	uint32_t has_size:1;
	/**<
	 * Flag to indicate if the size field is present
	 */
	uint32_t notify_rndv_ack:1;
	/**<
	 * Tells the FW to program the NIC to increment the NIC's QMAN fence counter
	 * when the RNDV write ack is received.
	 */
	uint32_t wait_for_rndv_acks:1;
	/**<
	 * Tells the FW to push a fence command to the QMAN and wait for all the
	 * previous rndv acks.
	 */
	uint32_t coll_ctxt_id:4;
	/**<
	 * Collective context ID to be used
	 */
	uint32_t nic_opcode:1;
	/**<
	 * NIC opcode : The value of this should always be 1,
	 * so that the 4 bit opcode becomes NIC_CMD_COLL_OPSx
	 * Remaining 3 bits are dont care for COLL_OPS operation
	 */
	uint32_t comm_desc_index:4;
	/**<
	 * index into nic_comm_desc_t
	 */
	uint32_t buffer_addr_lsb:28;
	/**<
	 * LSB address to send to. MSB is taken from collective context
	 */
	uint32_t buffer_size[0];
	/**<
	 * This field is not present always and is indicated by has_size flag.
	 * buffer_size if indicated by has_size flag
	 * nic_size = (cache_line_count * 3 * NIC_CACHE_LINE_SIZE_IN_ELEMENTS -
	 * 	(cache_line_remainder * NIC_CACHE_LINE_SIZE_IN_ELEMENTS) -
	 * 	element_remainder) * <number of devices in communicator>;
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_COLL_OPS_SHORT_SIZE (sizeof(struct arc_cmd_coll_ops_short_t))
#define ARC_CMD_COLL_OPS_SHORT_SIZE_DWORD (ARC_CMD_COLL_OPS_SHORT_SIZE / 4)

/**
 * \struct  arc_cmd_coll_ops_recv_short_inorder_v2_t
 * \brief   Perform send/recv short operation on NICs in exact order
 * \details This command is used to perform send/recv short operations by user.
 *	    This command is used for reproduceable reduction feature as it does
 *	    copying of data in exact same order on every invocation.
 */
struct arc_cmd_coll_ops_recv_short_inorder_v2_t {
	struct {
		uint32_t cache_line_count:15;
		/**<
		 * Amount of data in multiples of cache line size that each
		 * NIC needs to receive. This field is used only to decide the
		 * receieve buffer address. Sub Nic 0 receives at an offset 0
		 * Sub Nic 1 receieves at an offset of cache_line_count and
		 * Sub Nic 2 receives at and offset of cache_line_count * 2.
		 * In this case the cache_line_remainder and element_remainder
		 * fields are not needed because we are doing only receieve and
		 * only the Sender Sub Nic 2 sends partial data, so the offset
		 * for receiever Sub Nic 2 is always on cache_line_count boundary.
		 */
		uint32_t use_sob_inc_delta:1;
		/**<
		 * If use_sob_inc_delta is set, increment SOB ID of collective context by delta,
		 * else increment only once
 		 */
		uint32_t reserved:3;
		/**<
		 * Bitmap which indicates which subranks needs to only do
		 * signaling without doing actual receive
		 * TODO: Remove this field and move rndv flags here after
		 * 0 length NIC transfers are allowed
		 */
		uint32_t local_rank_index:3;
		/**<
		 * All the Ranks which are larger than the local rank should
		 * shift the (rank - 1) buffer offset before copying the data
		 */
		uint32_t comm_desc_index:4;
		/**<
		 * index into nic_comm_desc_t
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
		uint32_t nic_opcode:5;
		/**<
		 * NIC opcode : NIC_CMD_COLL_OPS_RECV_INORDER_V2
		 */
	};
	struct {
		uint32_t coll_ctxt_id:4;
		/**<
		 * Collective context ID to be used
		 */
		uint32_t reserved2:4;
		/**<
		 * Static Intermdiate Buffer for accumulation Index
		 */
		uint32_t sibo_index:9;
		/**<
		 * Static Intermediate Buffer for Ordering Index
		 * to calculate the source buffer address
		 */
		uint32_t reserved3:3;
		/**<
		 * Number of ranks that should be received in SIB Accumulation buffer
		 * Starting from 0, these many ranks stores data into SIB Order buffer
		 */
		uint32_t reduction_opcode:10;
		/**<
		 * Reduction parameters to be used when accumulating data into
		 * SIB Order buffer. For the rest of the NICs it uses Reduction
		 * parameters from Coll Context
		 * bit [0]:   Reduction indication
		 * bit [4-1]: Reduction data type
		 * bit [6-5]: Reduction operation
		 * bit [8-7]: Reduction rounding mode
		 * bit [9]:   Reduction Operation
		 */
		uint32_t notify_rndv_ack:1;
		/**<
		 * Tells the FW to program the NIC to increment the NIC's QMAN fence counter
		 * when the RNDV write ack is received.
		 */
		uint32_t wait_for_rndv_acks:1;
		/**<
		 * Tells the FW to push a fence command to the QMAN and wait for all the
		 * previous rndv acks.
		 */
	};
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_COLL_OPS_SHORT_INORDER_V2_SIZE (sizeof(struct arc_cmd_coll_ops_recv_short_inorder_v2_t))
#define ARC_CMD_COLL_OPS_SHORT_INORDER_V2_SIZE_DWORD (ARC_CMD_COLL_OPS_SHORT_INORDER_V2_SIZE / 4)

/**
 * \struct  arc_cmd_coll_ops_long_t
 * \brief   Perform send/recv long operation on NICs
 * \details This command is used to perform send/recv operations by user.
 *          Please refer to the details in arc_cmd_coll_ops_short_t for
 *          further details about size calculation
 */
struct arc_cmd_coll_ops_long_t {
	uint32_t addr_msb:13;
	/**<
	 * Starting from Bit32 onwards, 13 bits
	 */
	uint32_t comm_desc_index:4;
	/**<
	 * index into nic_comm_desc_t
	 */
	uint32_t cache_line_remainder:3;
	/**<
	 * Remainder to be subtracted from cache_line_count value to calculate
	 * the size of the data to be sent by NIC.
	 * ((ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ? 0 : 3 - (ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
	 */
	uint32_t force_remote_rank_offset:1;
	/**<
	 * This flag is used to force rank-offset to 0 during data transfer. This is not used for
	 * nic_size calculation and is applied after that. This means each sub nic will calculate
	 * offset only for within the rank but not between remote ranks.
	 */
	uint32_t coll_ctxt_id:4;
	/**<
	 * Collective context ID to be used
	 */
	uint32_t has_size:1;
	/**<
	 * Flag to indicate if the size field is present
	 */
	uint32_t has_stride:1;
	/**<
	 * Flag to indicate if the stride field is present
	 */
	uint32_t nic_opcode:5;
	/**<
	 * NIC opcode : The value of this should always be 1,
	 * so that the 4 bit opcode becomes NIC_CMD_COLL_OPSx
	 * Remaining 3 bits are dont care for COLL_OPS operation
	 */
	uint32_t buffer_addr_lsb;
	/**<
	 * LSB address to send to. MSB is taken from collective context
	 */
	uint32_t use_sob_inc_delta:1;
	/**<
	 * If use_sob_inc_delta is set, increment SOB ID of collective context by delta,
	 * else increment only once
	 */
	uint32_t element_remainder:6;
	/**<
	 * remainder in terms of number of elements when the data is not integer
	 * multiple of cache line. Typically used by the last nic in the sub group.
	 * ((cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS) == 0) ? NIC_CACHE_LINE_SIZE_IN_ELEMENTS - (cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS)
	 *
	 * where cell_size is in terms of number of elements and it represents total
	 * elements that need to be transferred
	 */
	uint32_t notify_rndv_ack:1;
	/**<
	 * Tells the FW to program the NIC to increment the NIC's QMAN fence counter
	 * when the RNDV write ack is received.
	 */
	uint32_t wait_for_rndv_acks:1;
	/**<
	 * Tells the FW to push a fence command to the QMAN and wait for all the
	 * previous rndv acks.
	 */
	uint32_t cache_line_count:23;
	/**<
	 * Amount of data in multiples of cache line size that each
	 * NIC needs to send.
	 * Value equal to ceil(ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
	 */
	uint32_t buffer_size[0];
	/**<
	 * This field is not present always and is indicated by has_size flag.
	 * buffer_size if indicated by has_size flag
	 * When this field is not present buffer size is calculated as
	 * nic_size = (cache_line_count * 3 * NIC_CACHE_LINE_SIZE_IN_ELEMENTS -
	 * 	(cache_line_remainder * NIC_CACHE_LINE_SIZE_IN_ELEMENTS) -
	 * 	element_remainder) * <number of devices in communicator>;
	 */
	uint32_t shuffle_stride[0];
	/**<
	 * This field is not present always and is indicated by has_stride
	 * flag. Buffer stride if indicated by the has_stride flag.
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_COLL_OPS_LONG_SIZE (sizeof(struct arc_cmd_coll_ops_long_t))
#define ARC_CMD_COLL_OPS_LONG_SIZE_DWORD (ARC_CMD_COLL_OPS_LONG_SIZE / 4)

/**
 * \struct  arc_cmd_send_recv_short_t
 * \brief   Perform send/recv operation on NICs
 * \details This command is used to perform send/recv operations by user.
 *          Please refer to the details in arc_cmd_coll_ops_short_t for
 *          further details about size calculation
 */
struct arc_cmd_send_recv_short_t {
	uint32_t addr_msb:18;
	/**<
	 * 18 bits of address MSB. First 14 bits are taken from
	 * coll_ctxt.addr_msb
	 */
	uint32_t comm_desc_index:4;
	/**<
	 * Communicator descriptor index in QPN table
	 */
	uint32_t datatype_size:2;
	/**<
	 * specifies size of element:
	 * datatype_size = 0 for unit8, int8
	 * datatype_size = 1 for int16, uint16, fp16, bf16
	 * datatype_size = 2 for fp32, int32, uint32
	 * datatype_size = 3 is unused
	 */
	uint32_t sob_increment:1;
	/**<
	 * Flag to increment sync object
	 */
	uint32_t use_sob_inc_delta:1;
	/**<
	 * If use_sob_inc_delta is set, increment SOB ID of collective context by delta,
	 * else increment only once
 	 */
	uint32_t coll_ctxt_id:4;
	/**<
	 *  Collective context ID to be used for send/recv
	 */
	uint32_t nic_opcode:2;
	/**<
	 * opcode of the command = NIC_CMD_SEND_RECV
	 */
	uint32_t addr_lsb:32;
	/**<
	 * LSB of 64 bit address
	 */
	uint32_t notify_rndv_ack:1;
	/**<
	 * Tells the FW to program the NIC to increment the NIC's QMAN fence counter
	 * when the RNDV write ack is received.
	 */
	uint32_t wait_for_rndv_acks:1;
	/**<
	 * Tells the FW to push a fence command to the QMAN and wait for all the
	 * previous rndv acks.
	 */
	uint32_t cache_line_count:21;
	/**<
	 * Amount of data in multiples of cache line size that each
	 * NIC needs to send.
	 * Value equal to ceil(ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
	 */
	uint32_t cache_line_remainder:3;
	/**<
	 * Remainder to be subtracted from cache_line_count value to calculate
	 * the size of the data to be sent by NIC.
	 * ((ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ? 0 : 3 - (ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
	 */
	uint32_t element_remainder:6;
	/**<
	 * remainder in terms of number of elements when the data is not integer
	 * multiple of cache line. Typically used by the last nic in the sub group.
	 * ((cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS) == 0) ? NIC_CACHE_LINE_SIZE_IN_ELEMENTS - (cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS)
	 *
	 * where cell_size is in terms of number of elements and it represents total
	 * elements that need to be transferred
	 */
}  __attribute__ ((aligned(4), __packed__));

enum {
	ARC_CMD_SEND_RECV_SHORT_SIZE = (sizeof(struct arc_cmd_send_recv_short_t)),
	ARC_CMD_SEND_RECV_SHORT_SIZE_DWORD = (ARC_CMD_SEND_RECV_SHORT_SIZE / 4),
};

/**
 * \struct  arc_cmd_nic_send_recv_nop_t
 * \brief   Perform release queue credits on NICs
 * \details This command is used to release queue credits
 *          on NIC
 */
struct arc_cmd_nic_send_recv_nop_t {
	uint32_t reserved:11;
	uint32_t comm_desc_index:4;
	/**<
	 * Communicator descriptor index in QPN table
	 */
	uint32_t queue_credits_bytes:6;
	/**<
	 * queue credits in bytes to be released by
	 * this command. This includes credits for
	 * NOP command
	 */
	uint32_t sob_increment:1;
	/**<
	 * Flag to increment sync object
	 */
	uint32_t use_sob_inc_delta:1;
	/**<
	 * If use_sob_inc_delta is set, increment SOB ID of collective context by delta,
	 * else increment only once
	 */
	uint32_t coll_ctxt_id:4;
	/**<
	 * Collective context ID to be used for send/recv
	 */
	uint32_t nic_opcode:5;
	/**<
	 * opcode of the command = NIC_CMD_SEND_RECV_NOP
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_SEND_RECV_NOP_SIZE (sizeof(struct arc_cmd_nic_send_recv_nop_t))
#define ARC_CMD_NIC_SEND_RECV_NOP_SIZE_DWORD (ARC_CMD_NIC_SEND_RECV_NOP_SIZE / 4)

/**
 * \enum    eng_arc_cmd_t
 * \brief   Various engine commands
 * \details Command type enum specifies the commands that are processed by
 *	    compute engines ARCs.
 */
enum edma_eng_arc_cmd_t {
	NIC_EDMA_CMD_SIBO_OPS_V3 = 0,
	NIC_EDMA_CMD_LIN_OPS_V3 = 1,
	NIC_EDMA_CMD_SIBO_MEMSET_V3 = 2,
	NIC_EDMA_CMD_UNUSED3 = 3,
	NIC_EDMA_CMD_UNUSED4 = 4,
	NIC_EDMA_CMD_UNUSED5 = 5,
	NIC_EDMA_CMD_LIN_MEMSET_V3_2 = 6,
	NIC_EDMA_CMD_Q_SEL = 7,
	NIC_EDMA_CMD_UPDATE_GLBL_CTXT_V3 = 8,
	NIC_EDMA_CMD_COUNT = 9,
	NIC_EDMA_COUNT = NIC_EDMA_CMD_COUNT, /* Depricated, dont use */
};

/**
 * \enum    nic_edma_datasizes_t
 * \brief   edma operation data type
 * \details Data type for edma operations, local datatype, output datatype
 */
enum nic_edma_datasizes_t {
	NIC_EDMA_8BITS = 0x0,
	NIC_EDMA_16BITS = 0x1,
	NIC_EDMA_32BITS = 0x2,
	NIC_EDMA_DSIZE_MAX = 0x3
};

/**
 * \enum    nic_edma_datatypes_t
 * \brief   edma operation data type
 * \details Data type for edma operations, local datatype, output datatype
 */
enum nic_edma_datatypes_t {
	NIC_EDMA_UNSIGNED = 0x0,
	NIC_EDMA_SIGNED = 0x1,
	NIC_EDMA_FP = 0x2,
	NIC_EDMA_BF = 0x3,
	NIC_EDMA_DTYPE_MAX = 0x4
};

/**
 * \struct  arc_cmd_nic_edma_sibo_ops_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_sibo_ops_v3_t {

	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_base:3;
		/**<
		 * index of SOB base in glbl ctxt that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t sob_index:10;
		/**<
		 * SOB index from comp_cfg base that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t signal_second:1;
		/**<
		 * Whether second_signal is needed or not
		 */
		uint32_t second_sob_base:3;
		/**<
		 * index of SOB base in glbl ctxt that needs to be used for signaling second
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t second_sob_index:10;
		/**<
		 * SOB index from comp_cfg base that needs to be used for signaling second
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t sibo_index:9;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:3;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t local_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t sibo_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t output_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */

		uint32_t :4;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low for the local buffer
	 */
	struct {
		uint32_t src_addr_hi:24;
		/**<
		 * Source address hi for the local buffer
		 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
		 */
		uint32_t reduction_ind:1;
		/**<
		 * Reduction indication
		 */
		uint32_t reduction_in_place:1;
		/**<
		 * TODO: This flag is kept for future use.
		 * We haven't received clear requirements yet, but
		 * keeping this flag so that we dont forget
		 */
		uint32_t wide_accumulation:1;
		/**<
		 * When set reduction operation should happen in FP32 data type.
		 * All the inputs should be up casted to FP32 and copied
		 * to output after appropriate conversion based on output
		 * data type
		 */
		uint32_t reduction_op:3;
		/**<
		 * Reduction operation to be performed
		 */
		uint32_t dtype:2;
		/**<
		 * Type of data, which includes unsigned, singed, FP, and BF.
		 */
	} __attribute__ ((aligned(4), __packed__));
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE (sizeof(struct arc_cmd_nic_edma_sibo_ops_v3_t))
#define ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_SIBO_OPS_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_lin_ops_v3_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_lin_ops_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t input_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t output_datasize:2;
		/**<
		 * 0 - 8bit
		 * 1 - 16bit
		 * 2 - 32bit
		 */
		uint32_t dtype:2;
		/**<
		 * Type of data, which includes unsigned, signed, FP, and BF.
		 * 0 -> unsigned
		 * 1 -> signed
		 * 2 -> FP16
		 * 3 -> BF16
		 * refer to nic_edma_datatypes_t
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :19;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t src_addr_lo;
	/**<
	 * Source address low for the local buffer
	 */
	struct {
		uint32_t src_addr_hi:24;
		/**<
		 * Source address hi for the local buffer
		 * src_addr_hi[31:25] are taken from dst_addr_hi[31:25]
		 * TODO: Can we reduce this to 8 Bits ? 96GB of Cache
		 */
		uint32_t reduction_ind:1;
		/**<
		 * Reduction indication
		 */
		uint32_t reduction_in_place:1;
		/**<
		 * TODO: This flag is kept for future use.
		 * We haven't received clear requirements yet, but
		 * keeping this flag so that we dont forget
		 */
		uint32_t reduction_op:2;
		/**<
		 * Reduction operation to be performed
		 */
		uint32_t :4;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE (sizeof(struct arc_cmd_nic_edma_lin_ops_v3_t))
#define ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_LIN_OPS_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_sibo_memset_v3_t
 * \brief   Perform Memset operation on SIBO buffer
 * \details Perform Memset operation on SIBO buffer
 *	    associated with NICs. HCL team uses this to clear buffers
 *          partially in SIBO , thats why we cant just use linear memset
 *          instead of this.
 */
struct arc_cmd_nic_edma_sibo_memset_v3_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_address:27;
		/**<
		 * SOB address that needs to be used for signaling completion
		 * address = 0xF8000000 | sob_address;
		 */
		uint32_t :1;
		/**<
		 */
	} __attribute__ ((aligned(4), __packed__));

	struct {
		uint32_t sibo_index:9;
		/**<
		 * SIB Order buffer Index to calculate the source buffer address
		 */
		uint32_t rank_offset_in_sibo:3;
		/**<
		 * Rank offset in SIB order buffer to start with
		 */
		uint32_t rank_count:7;
		/**<
		 * Number of ranks to be used as input buffer
		 */
		uint32_t pool_id:1;
		/**<
		 * pool id to select which intermediate buffer to be used
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :5;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t memset_value;
	/**<
	 * Value to be used for memset
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE (sizeof(struct arc_cmd_nic_edma_sibo_memset_v3_t))
#define ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE_DWORD (ARC_CMD_NIC_EDMA_SIBO_MEMSET_V3_SIZE / 4)

/**
 * \struct  arc_cmd_nic_edma_lin_memset_v3_2_t
 * \brief   Perform various EDMA operations
 * \details Perform various EDMA operations
 *	    associated with NICs.
 */
struct arc_cmd_nic_edma_lin_memset_v3_2_t {
	struct {
		uint32_t opcode:4;
		/**<
		 * opcode of the command
		 */
		uint32_t sob_base:3;
		/**<
		 * index of SOB base in glbl ctxt that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t sob_index:10;
		/**<
		 * SOB index from comp_cfg base that needs to be used for signaling first
		 * completion address = comp_cfg[sob_base] + sob_index * 4;
		 */
		uint32_t context_id:7;
		/**<
		 * context id for profiler trace
		 */
		uint32_t :8;
		/**<
		 * unused
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t transfer_size;
	/**<
	 * transfer size in bytes
	 */
	uint32_t dst_addr_lo;
	/**<
	 * Destination address lo
	 */
	uint32_t dst_addr_hi;
	/**<
	 * Destination address high
	 */
	uint32_t memset_value;
	/**<
	 * Value to be used for memset
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_NIC_EDMA_LIN_MEMSET_SIZE (sizeof(struct arc_cmd_nic_edma_lin_memset_v3_2_t))
#define ARC_CMD_NIC_EDMA_LIN_MEMSET_SIZE_DWORD (ARC_CMD_NIC_EDMA_LIN_MEMSET_SIZE / 4)

#define FP32_MAX_POS_VAL	0x7F800000
#define FP32_MAX_NEG_VAL	0xFF800000

/**
 * \struct  nic_scaleout_coll_ctxt_t
 * \brief   Collective context for scaleout NICs
 * \details Each NIC engine ARC Firmware maintains NIC_SCALEOUT_COLL_CTXT_COUNT
 *	    collective contexts which can be independently updated by
 *	    HCL.
 */
struct nic_scaleout_coll_ctxt_t {
	union {
		struct {
			union {
				struct {
					uint16_t reduction_opcode;
					/**<
					 * This field is part of DWORD0.
					 * Reduction opcode including SE, in_line and ackreq
					 * fields thats why it is made 16bits
					 */
					uint16_t reserved2;
					/**<
					 * This field is part of DWORD0.
					 */
				} __attribute__ ((aligned(4), __packed__));
				uint32_t raw_dword0;
			};
			uint32_t sync_object_address_0;
			/**<
			 * This field is part of DWORD1.
			 * LBW address of the SOB that should be used by Firmware
			 * to program into NIC
			 * Engine ARC Firmware moves to the next SOB address
			 * based on the sob_index field of SCHED_ARC_CMD_NIC_COLL_OPS command
			 * sob address 0 or 1 is picked.
			 */
			uint32_t sync_object_address_1;
			/**<
			 * This field is part of DWORD2.
			 */
			uint32_t buffer_addr_msb;
			/**<
			 * This field is part of DWORD3.
			 * Upper 32bits of the buffer.
			 */
			uint32_t stride;
			/**<
			 * This field is part of DWORD4.
			 * Stride to be used for a rank in the buffer
			 */
		} __attribute__ ((aligned(4), __packed__));
		uint32_t dword[5];
	};
} __attribute__ ((aligned(4), __packed__));

struct nic_coll_ops_scaleout_qpn_desc_t {
	uint16_t remote_scaleout_index;
	/**<
	 * Index of remote scaleout NIC
	 */
	uint16_t qpn_subnic[3];
	/**<
	 * QPN for subnic 0, 1 and 2
	 */

} __attribute__ ((aligned(4), __packed__));

struct nic_coll_ops_scaleout_qpn_desc_v2_t {
	uint16_t remote_scaleout_index;
	/**<
	 * Index of remote scaleout NIC
	 */
	uint16_t qpn_subnic[6];
	/**<
	 * QPN for subnics 0 to 5
	 */
	uint16_t reserved;
	/**<
	 * reserved
	 */
} __attribute__ ((aligned(4), __packed__));

/**
 * \struct  arc_cmd_coll_ops_scaleout_t
 * \brief   Perform send/recv operation on scaleout NICs
 * \details This command is used to perform send/recv short operations by user.
 *          Following is an example of how the values are calculated:
 *          Total transfer = cell_size = 1023 elements
 *          transfer size = 4092 if we consider FP32 as datatype
 *          cache line - 64 (does not depend on data type)
 *          cache_line_count having 64 elements each = 6 ciel(ciel(1023/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
 *          cache_line_remainder = 2 ((ceil(1023/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ?
 *              0 : 3 - (ceil(cell_size/NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
 *          element_remainder in number of elements = 1 (1 element of FP32)
 *
 *          while transferring data:
 *          NIC0 or NIC1
 *              nic_size = (6 * 64) * data_type;
 *          NIC2
 *              nic_size = (((6 - 2) * 64) - element_remainder) * data_type;
 *
 *          Note: There is a special case when cache_line_count <= 3. In that case entire data is
 *          sent by NIC0 and other NICs do not perform any transaction. They however still update
 *          the SOBs
 */
struct arc_cmd_coll_ops_scaleout_t {
	struct {
		uint32_t update_bitmask:5;
		/**<
		 * Bitmask for collective ctxt
		 */
		uint32_t num_dwords_bitmask:3;
		/**<
		 * Number of dwords to be updated by this bitmask
		 */
		uint32_t is_qpn_desc_v2:1;
		/**<
		 * 0 - qpn desc is of nic_coll_ops_scaleout_qpn_desc_t
		 * 1 - qpn desc is of nic_coll_ops_scaleout_qpn_desc_v2_t
		 */
		uint32_t reserved:3;
		/**<
		 * Reserved
		 */
		uint32_t notify_rndv_ack:1;
		/**<
		 * Tells the FW to program the NIC to increment the NIC's QMAN fence counter
		 * when the RNDV write ack is received.
		 */
		uint32_t wait_for_rndv_acks:1;
		/**<
		 * Tells the FW to push a fence command to the QMAN and wait for all the
		 * previous rndv acks.
		 */
		uint32_t qpn_desc_count:5;
		/**<
		 * Count of QPN descriptors received as a part of command
		 */
		uint32_t cache_line_remainder:3;
		/**<
		 * Remainder to be subtracted from cache_line_count value to calculate
		 * the size of the data to be sent by NIC.
		 * ((ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3) == 0) ?
		 *     0 : 3 - (ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) % 3)
		 */
		uint32_t coll_ctxt_id:4;
		/**<
		 * Collective context ID to be used
		 */
		uint32_t sob_index:1;
		/**<
		 * Sync Object to be used by this command
		 * - 0 : SO = 0 from collective context
		 * - 1 : SO = 1 from collective context
		 */
		uint32_t has_size:1;
		/**<
		 * Flag to indicate if the size field is present
		 */
		uint32_t nic_opcode:4;
		/**<
		 * NIC opcode
		 */
	};
	struct {
		uint32_t element_remainder:6;
		/**<
		 * remainder in terms of number of elements when the data is not integer
		 * multiple of cache line. Typically used by the last nic in the sub group.
		 * ((cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS) == 0) ?
		 *   NIC_CACHE_LINE_SIZE_IN_ELEMENTS - (cell_size % NIC_CACHE_LINE_SIZE_IN_ELEMENTS)
		 *
		 * where cell_size is in terms of number of elements and it represents total
		 * elements that need to be transferred
		 */
		uint32_t cache_line_count:26;
		/**<
		 * Amount of data in multiples of cache line size that each
		 * NIC needs to send.
		 * Value equal to ceil(ceil(cell_size / NIC_CACHE_LINE_SIZE_IN_ELEMENTS) / 3)
		 */
	};
	uint32_t buffer_addr_lsb;
	/**<
	 * LSB address to send to. MSB is taken from collective context
	 */
	uint32_t buffer_size[0];
	/**<
	 * This field is not present always and is indicated by has_size flag.
	 * buffer_size if indicated by has_size flag
	 * nic_size = (cache_line_count * 3 * NIC_CACHE_LINE_SIZE_IN_ELEMENTS -
	 * 	(cache_line_remainder * NIC_CACHE_LINE_SIZE_IN_ELEMENTS) -
	 * 	element_remainder) * <number of devices in communicator>;
	 */
	uint32_t dword_value[0];
	/**<
	 * This field is not present always and is indicated by update_bitmask field.
	 * If present this contains the dwords of collective ctxt structure
	 */
	struct nic_coll_ops_scaleout_qpn_desc_t qpn_desc[0];
	/**<
	 * This field is not present always and is indicated by qpn_desc_count field.
	 */
	struct nic_coll_ops_scaleout_qpn_desc_v2_t qpn_desc_v2[0];
	/**<
	 * This is to send qpn descriptor for scaleout upto 6 NIC ports
	 * This field is not present always and is indicated by qpn_desc_count field.
	 */
}  __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_COLL_OPS_SCALEOUT_SIZE (sizeof(struct arc_cmd_coll_ops_scaleout_t))

/**
 * \struct  arc_cmd_update_edma_nic_ctxt_v3_t
 * \brief   Update the ctxt for NIC-EDMAs
 * \details Update the ctxt for NIC-EDMAs
 */
struct arc_cmd_update_edma_nic_ctxt_v3_t {
	uint32_t opcode:4;
	/**<
	 * opcode of the command = NIC_EDMA_CMD_UPDATE_GLBL_CTXT_V3
	 */
	uint32_t update_bitmap:18;
	/**<
	 * Bitmap of the DWORDs, which needs to be updated
	 */
	uint32_t reserved:5;
	/**<
	 * reserved
	 */
	uint32_t num_dwords:5;
	/**<
	 * Number of DWORDS to be updated by this command
	 */
	struct {
		uint32_t sob_address:28;
		/**<
		 * SOB address that needs to be used for signaling completion
		 *
		 * If this value is 0, then Signalling will not happen
		 */
		uint32_t rsvd:4;
		/**<
		 * reserved
		 */
	} __attribute__ ((aligned(4), __packed__));

	uint32_t data[0];
	/**<
	 * Actual DWORDS data to be updated by this command
	 */
} __attribute__ ((aligned(4), __packed__));

#define ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE (sizeof(struct arc_cmd_update_edma_nic_ctxt_v3_t))
#define ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE_DWORD (ARC_CMD_UPDATE_EDMA_NIC_CTXT_V3_SIZE / 4)
#define ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE (sizeof(struct arc_cmd_update_edma_nic_ctxt_v3_t) +	\
						(EDMA_NIC_MAX_DWORDS_IN_CTXT_V3 * sizeof(uint32_t)))
#define ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE_DWORD (ARC_CMD_UPDATE_EDMA_NIC_MAX_CTXT_V3_SIZE / 4)

#endif /* __GAUDI2_ARC_ENG_PACKETS_H__ */
